{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e5665d-647a-4a55-8e17-8c9c11a7626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import stanza\n",
    "import re\n",
    "import string\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d4a0cc-67b4-4d5f-b1bb-0d28007456f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fda8f2e-4913-4141-a1d0-1de97ef368ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1409</td>\n",
       "      <td>11230271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>2528</td>\n",
       "      <td>47140692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>3603</td>\n",
       "      <td>66010962</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>4768</td>\n",
       "      <td>88051252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>5325</td>\n",
       "      <td>97514182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>6156</td>\n",
       "      <td>109542962</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>6369</td>\n",
       "      <td>112971462</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>6474</td>\n",
       "      <td>114817972</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>7006</td>\n",
       "      <td>121659062</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>7504</td>\n",
       "      <td>126644672</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   tweet_id text\n",
       "671         1409   11230271  NaN\n",
       "1132        2528   47140692  NaN\n",
       "1628        3603   66010962  NaN\n",
       "2298        4768   88051252  NaN\n",
       "2660        5325   97514182  NaN\n",
       "3175        6156  109542962  NaN\n",
       "3285        6369  112971462  NaN\n",
       "3344        6474  114817972  NaN\n",
       "3737        7006  121659062  NaN\n",
       "4075        7504  126644672  NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9b6328-876a-45e3-8320-1a963ac0348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['tweet_id']==11230271]['text'].isna().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588ee228-fdec-4f91-810a-5b1324e50a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639ffbb1d47041a095daf3cb3af8540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 15:06:02 INFO: Downloading default packages for language: it (Italian) ...\n",
      "2023-11-30 15:06:03 INFO: File exists: /g100/home/userexternal/ddurmush/stanza_resources/it/default.zip\n",
      "2023-11-30 15:06:08 INFO: Finished downloading models and saved to /g100/home/userexternal/ddurmush/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84eda767-832e-4af7-9ab9-73f539457a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for future how to display whole tweets \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b425e42-1dd8-4a4d-a0be-aa4149220668",
   "metadata": {},
   "outputs": [],
   "source": [
    "### just a random dataset to analyze\n",
    "with open('/g100_work/IscrC_mental/data/user_classification/user_age_gender_location_test_set.pkl', 'rb') as file:\n",
    "    data_test = pickle.load(file)\n",
    "tweets = list(data_test['tweet'])\n",
    "tweets = [str(tw) for tw in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad2ed852-22d1-45c2-baff-911f0e82f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "\n",
    "components_to_remove = ['tagger', 'parser', 'attribute_ruler', 'ner']\n",
    "for component in components_to_remove:\n",
    "    it_nlp_spacy.remove_pipe(component)\n",
    "    \n",
    "    \n",
    "it_nlp_stanza = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56006402-3b87-4cdd-be20-9b631eba72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "stop_words_to_remove = ['anni', 'anno']\n",
    "stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in text.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "    filtered_text = ' '.join(words_new)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d01af01-388b-42b0-a4ba-5007cb341a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0e8c10-6783-4b92-989f-daf4a69ee5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StefanoGuerrera figlia piccola insegnato baciare labbra fatto 30 anni madre facevo affetto c’è rimpiango vergognata problemi sessualità \n"
     ]
    }
   ],
   "source": [
    "print(cleaning(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b41f5fd-9cbc-4874-8691-c55945ba2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizationSpacy(text):\n",
    "\n",
    "    tokens = it_nlp_spacy(text)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    \n",
    "    return lemmatized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a91efcb7-7232-40f8-afb7-66fb1a746e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StefanoGuerrera figlio piccolo insegnato baciare labbra fatto 30 anno madre facevo affetto ci essere rimpiango vergognare problema sessualità\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizationSpacy(cleaning(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a4d85b-316f-488a-92c3-28ee5f6fd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizationStanza(text):\n",
    "    doc = it_nlp_stanza(text)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    \n",
    "    return lemmatized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ec1598-4587-4735-8d83-af03d75800b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StefanoGuerrera  figlio  piccolo  insegnare  baciare  labbro  fare  30  anno  madre  fare  affetto  ci  essere  rimpiangere  vergognato  problema  sessualità \n"
     ]
    }
   ],
   "source": [
    "print(lemmatizationStanza(cleaning(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbc7194-866d-4512-b916-fcc385b2db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_from_db(chunk_size):\n",
    "    \n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"SELECT tweet_id, text FROM tweets LIMIT {chunk_size}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Create a DataFrame from the fetched rows\n",
    "    chunk_df = pd.DataFrame(rows, columns=['tweet_id', 'text'])\n",
    "    \n",
    "    return chunk_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc9bc5a-8561-48e5-92e0-6810b390dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(f\"SELECT tweet_id, text, language FROM tweets LIMIT 1000\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Create a DataFrame from the fetched rows\n",
    "chunk_df = pd.DataFrame(rows, columns=['tweet_id', 'text', 'lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d7ac63-7e6a-4d6b-ae14-3a4adfef845b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53366</td>\n",
       "      <td>I'm in NY working on some cool stuff! ;-P</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94933</td>\n",
       "      <td>www</td>\n",
       "      <td>qst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>614113</td>\n",
       "      <td>Now I'll begin to work.\\r\\nAnd i've just signed on twitter.\\r\\nHallo to everybody!\\r\\n</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>661263</td>\n",
       "      <td>I'll begin to work now, like jesterday.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666033</td>\n",
       "      <td>I've just had a coffee.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5476148</td>\n",
       "      <td>News: Microsoft, Novell Announce First Road Map http://tinyurl.com/37xarp</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5476177</td>\n",
       "      <td>News: http://tinyurl.com/2smfv6 Meridiana comunica di avere messo in vendita un tot di biglietti per Pasqua (periodo: 2-11 aprile) a partire</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5476214</td>\n",
       "      <td>News: http://tinyurl.com/2hsym2 In occasione della festa della donna del 8 marzo Evolavia fa pagare alle donne solo le tasse, mentre l'accom</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5476255</td>\n",
       "      <td>News: http://tinyurl.com/254k9x Firefox has reached a new milestone: 300,000,000 downloads since its initial release back in November 9, 200</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5476289</td>\n",
       "      <td>News: http://tinyurl.com/yss8y6 What’s all this, then? Nokia’s new Video Center offers RSS video feeds along with content directly from YouT</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id  \\\n",
       "0       53366   \n",
       "1       94933   \n",
       "2      614113   \n",
       "3      661263   \n",
       "4      666033   \n",
       "..        ...   \n",
       "995   5476148   \n",
       "996   5476177   \n",
       "997   5476214   \n",
       "998   5476255   \n",
       "999   5476289   \n",
       "\n",
       "                                                                                                                                             text  \\\n",
       "0                                                                                                       I'm in NY working on some cool stuff! ;-P   \n",
       "1                                                                                                                                             www   \n",
       "2                                                          Now I'll begin to work.\\r\\nAnd i've just signed on twitter.\\r\\nHallo to everybody!\\r\\n   \n",
       "3                                                                                                         I'll begin to work now, like jesterday.   \n",
       "4                                                                                                                         I've just had a coffee.   \n",
       "..                                                                                                                                            ...   \n",
       "995                                                                     News: Microsoft, Novell Announce First Road Map http://tinyurl.com/37xarp   \n",
       "996  News: http://tinyurl.com/2smfv6 Meridiana comunica di avere messo in vendita un tot di biglietti per Pasqua (periodo: 2-11 aprile) a partire   \n",
       "997  News: http://tinyurl.com/2hsym2 In occasione della festa della donna del 8 marzo Evolavia fa pagare alle donne solo le tasse, mentre l'accom   \n",
       "998  News: http://tinyurl.com/254k9x Firefox has reached a new milestone: 300,000,000 downloads since its initial release back in November 9, 200   \n",
       "999  News: http://tinyurl.com/yss8y6 What’s all this, then? Nokia’s new Video Center offers RSS video feeds along with content directly from YouT   \n",
       "\n",
       "    lang  \n",
       "0     en  \n",
       "1    qst  \n",
       "2     en  \n",
       "3     en  \n",
       "4     en  \n",
       "..   ...  \n",
       "995   en  \n",
       "996   it  \n",
       "997   it  \n",
       "998   en  \n",
       "999   en  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3865f4-ecc7-4466-b8a5-1aab06f28298",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.to_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25f92f31-7de6-427e-84ae-e461776a43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k['text'] = k['text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f8b7d21-1da2-4de5-9405-4c0048505ed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cleaning() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     filtered_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words_new)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_text\n\u001b[0;32m---> 15\u001b[0m k[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mcleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cleaning() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "stop_words_to_remove = ['anni', 'anno']\n",
    "stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in text.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "    filtered_text = ' '.join(words_new)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "k['text'] = k['text'].apply(cleaning())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a3a9230-c83a-4e8e-970d-b41c43256750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tweet_id  \\\n",
      "0      53366   \n",
      "1      94933   \n",
      "2     614113   \n",
      "3     661263   \n",
      "4     666033   \n",
      "5     666103   \n",
      "6     827173   \n",
      "7     992993   \n",
      "8    1002833   \n",
      "9    1120053   \n",
      "10   1490743   \n",
      "11   1542453   \n",
      "12   1544283   \n",
      "13   1794873   \n",
      "14   1857393   \n",
      "15   1875613   \n",
      "16   1879593   \n",
      "17   1879973   \n",
      "18   1892333   \n",
      "19   1892983   \n",
      "\n",
      "                                                                                      text  \n",
      "0                                                I'm in NY working on some cool stuff! ;-P  \n",
      "1                                                                                      www  \n",
      "2   Now I'll begin to work.\\r\\nAnd i've just signed on twitter.\\r\\nHallo to everybody!\\r\\n  \n",
      "3                                                  I'll begin to work now, like jesterday.  \n",
      "4                                                                  I've just had a coffee.  \n",
      "5                                           I think in a few minute it will rain here.....  \n",
      "6                                          on work, despite today it would be holiday.....  \n",
      "7                                                                watching weather forecast  \n",
      "8                                mi preparo e poi accompagno mamma a fare regali di natale  \n",
      "9                                                  is selecting the best albums for 2006 !  \n",
      "10                                                at work, letting Mucio to get bored! ;-)  \n",
      "11                                                                      I'm going to lunch  \n",
      "12                                                         dressed up as a cosmushroom...   \n",
      "13                                  Listening to community 2.0 webcast from shared insight  \n",
      "14                                                                     Searching css table  \n",
      "15                                                               just signed up on twitter  \n",
      "16                                                                           Reading bogs!  \n",
      "17                                                           Testing Gtalk-twitter account  \n",
      "18                                                                        doing breakfast.  \n",
      "19                                         temporeggio aspettando qualcosa da fare..\\r\\r\\n  \n"
     ]
    }
   ],
   "source": [
    "print(get_tweets_from_db(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c558c4c-07cd-4a44-950f-61fa542b0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_db(\n",
    "        db_file,\n",
    "        table_name,\n",
    "        chunk_size,\n",
    "        max_tweets_in_file=1000000,\n",
    "        fout=None,\n",
    "        ):\n",
    "    \n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the total number of rows\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_rows, unit=\"row\", desc=\"Processing\")\n",
    "\n",
    "    n_saved_files=0\n",
    "    # Loop through the data in chunks\n",
    "    for offset in range(0, total_rows, chunk_size):\n",
    "        # Query the database for a chunk of rows\n",
    "        cursor.execute(f\"SELECT tweet_id, text FROM {table_name} LIMIT {chunk_size} OFFSET {offset}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Create a DataFrame from the fetched rows\n",
    "        chunk_df = pd.DataFrame(rows, columns=['tweet_id', 'text'])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(len(rows))\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(get_info_by_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb66d5-9f82-46d6-bc9f-094e6c08b783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61dbf0-108a-4e90-867d-8d4a10bfafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database connection\n",
    "conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM tweets\")\n",
    "total_rows = cursor.fetchone()[0]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a tqdm progress bar\n",
    "progress_bar = tqdm(total=total_rows, unit=\"row\", desc=\"Processing\")\n",
    "\n",
    "n_saved_files=0\n",
    "\n",
    "\n",
    "    # Loop through the data in chunks\n",
    "    for offset in range(0, total_rows, chunk_size):\n",
    "        # Query the database for a chunk of rows\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT {chunk_size} OFFSET {offset}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Create a DataFrame from the fetched rows\n",
    "        chunk_df = pd.DataFrame(rows, columns=column_names)\n",
    "        \n",
    "        # Remove the unwanted columns\n",
    "        if remove_columns is not None:\n",
    "            chunk_df = chunk_df.drop(columns=remove_columns)\n",
    "\n",
    "        # Append to the result DataFrame\n",
    "        # chunk_df['user_id'] = chunk_df['user_id'].astype(str)\n",
    "        result_df = pd.concat([result_df, chunk_df[chunk_df['user_id'].isin(user_ids_to_retrieve)]], ignore_index=True)\n",
    "\n",
    "        if len(result_df) >= max_tweets_in_file:\n",
    "            # Save results to pickle\n",
    "            print(f'Saving {len(result_df)} tweets to file {n_saved_files}')\n",
    "            result_df.to_pickle(f'{fout}_{n_saved_files}.pkl')\n",
    "            # Reset the result DataFrame\n",
    "            result_df = pd.DataFrame()\n",
    "            n_saved_files += 1\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(len(rows))\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(get_info_by_user_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e0deb-ab85-44f6-9acc-3ccb1c6cfda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870df6e8-b2e4-4b2d-a332-5c60854894ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc1fc5-ad71-4c3c-8c89-7af34e630553",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "\n",
    "components_to_remove = ['tagger', 'parser', 'attribute_ruler', 'ner']\n",
    "for component in components_to_remove:\n",
    "    it_nlp_spacy.remove_pipe(component)\n",
    "    \n",
    "it_nlp_stanza = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)\n",
    "\n",
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "stop_words_to_remove = ['anni', 'anno']\n",
    "stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in text.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "    filtered_text = ' '.join(words_new)\n",
    "    return filtered_text\n",
    "\n",
    "def lemmatizationSpacy(text):\n",
    "    tokens = it_nlp_spacy(text)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    return lemmatized_tweet\n",
    "\n",
    "def lemmatizationStanza(text):\n",
    "    doc = it_nlp_stanza(text)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    return lemmatized_tweet\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Customizable pipeline\")\n",
    "    parser.add_argument('--cleaning', action='store_true', help='Run cleaning')\n",
    "    parser.add_argument('--lemmatizationSpacy', action='store_true', help='Run lemmatization with SpaCy')\n",
    "    parser.add_argument('--lemmatizationStanza', action='store_true', help='Run lemmatization with Stanza')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.cleaning:\n",
    "        cleaning()\n",
    "\n",
    "    if args.lemmatizationSpacy:\n",
    "        lemmatizationSpacy()\n",
    "\n",
    "    if args.lemmatizationStanza:\n",
    "        lemmatizationStanza()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f2b82-c172-4488-bccb-08560444c629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ef160-e8de-4cc9-a84a-4783e62f0c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70d0b4-e575-4f05-8daa-084312d63eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "115f1fa5-7ca7-41d6-9471-9779645c13a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaning', 'tok2vec', 'morphologizer', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('it_core_news_lg')\n",
    "\n",
    "components_to_remove = ['tagger', 'parser', 'attribute_ruler', 'ner']\n",
    "for component in components_to_remove:\n",
    "    nlp.remove_pipe(component)\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"cleaning\")\n",
    "def cleaning(doc):\n",
    "    stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "    new_tokens = [token.text for token in doc if not \n",
    "                  (re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", token.text) or token.is_punct or token.text.lower() in stop_words)]\n",
    "    return spacy.tokens.Doc(doc.vocab, words=new_tokens)\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"cleaning\", name=\"cleaning\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df57d5c3-39e2-4530-83e9-e2d444b8b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tweets_spacy = []\n",
    "for tweet in tweets:\n",
    "    tokens = nlp(tweet)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    lemmatized_tweets_spacy.append(lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6889b003-aa31-414d-9e31-a5b25484be49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e5e166ac-c82d-4cce-bf69-05622aab7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check = pd.DataFrame({'original': tweets, 'lemmatized_spacy':lemmatized_tweets_spacy })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7acee6e8-0c4e-4693-9573-d31020291208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemmatized_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...</td>\n",
       "      <td>figlio piccolo insegnato baciare labbro 30 madre affetto ci rimpiango vergognare problema sessualità</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Per tutti i followers...... non sono il front man dei Dari, sono un informatico ed ho 42 anni, il Dario che cercate é un mio omonimo.</td>\n",
       "      <td>il followers front man Dari informatico 42 Dario cercare é omonimo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!</td>\n",
       "      <td>51 capitare provare odio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@carmelitadurso ciao barbara  ho ho seguito l'intervento di lemme io sono grassa ho 53 anni sono pronta a mettermi in gioco se mi seguì tu</td>\n",
       "      <td>ciao Barbara   intervento lemme grasso 53 pronto mettere mi gioco seguire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PietroF70 Diplomato alla scuola alberghiera,ho fatto aiuto cuoco, lavapiatti,portiere notturno e istruttore di scuola guida e perché ho 42 anni a casa</td>\n",
       "      <td>diplomato scuola alberghiero aiuto cuoco Lavapiatti portiere notturno istruttore scuola guidare 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>Mi chiamo Marianna, ho 28 anni e  vorrei credere nel mio voto. Ma ahimè, tristemente andrò a votare quello che fa meno schifo, forse.</td>\n",
       "      <td>chiare Marianna 28   volere credere voto tristemente andrò votare schifo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>@civati Populismo di sinistra, populismo di destraAggrappati all’unica vera figura seria, l’attuale presidente del consiglio.Non credo di aver mai letto/sentito da quando sono nato (e ho 47 anni):”facciamo come l’Italia” detto dai tedeschi. Ci vuole preparazione/cultura per fare politica</td>\n",
       "      <td>populismo sinistra populismo destraAggrappare a il unico vero figura serio il attuale presidente credere letto sentire nascere 47 anni):”faccare il Italia tedesco volere preparazione cultura politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>@rosita17rosita ho 49anni e vorrei diventare un politico senza compenso solo per risolvere i problemi dell'Italia</td>\n",
       "      <td>49anni volere politico compenso risolvere il problema Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                original  \\\n",
       "0     @StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...   \n",
       "1                                                                                                                                                                  Per tutti i followers...... non sono il front man dei Dari, sono un informatico ed ho 42 anni, il Dario che cercate é un mio omonimo.   \n",
       "2                                                                                                                                                                                                           @ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!   \n",
       "3                                                                                                                                                             @carmelitadurso ciao barbara  ho ho seguito l'intervento di lemme io sono grassa ho 53 anni sono pronta a mettermi in gioco se mi seguì tu   \n",
       "4                                                                                                                                                @PietroF70 Diplomato alla scuola alberghiera,ho fatto aiuto cuoco, lavapiatti,portiere notturno e istruttore di scuola guida e perché ho 42 anni a casa   \n",
       "...                                                                                                                                                                                                                                                                                                  ...   \n",
       "1114                                                                                                                                                               Mi chiamo Marianna, ho 28 anni e  vorrei credere nel mio voto. Ma ahimè, tristemente andrò a votare quello che fa meno schifo, forse.   \n",
       "1115    @civati Populismo di sinistra, populismo di destraAggrappati all’unica vera figura seria, l’attuale presidente del consiglio.Non credo di aver mai letto/sentito da quando sono nato (e ho 47 anni):”facciamo come l’Italia” detto dai tedeschi. Ci vuole preparazione/cultura per fare politica   \n",
       "1116                                                                                                                                                                                                                                                                                                 nan   \n",
       "1117                                                                                                                                                                                   @rosita17rosita ho 49anni e vorrei diventare un politico senza compenso solo per risolvere i problemi dell'Italia   \n",
       "1118                                                                                                                                                                                                                                                                                                 nan   \n",
       "\n",
       "                                                                                                                                                                                             lemmatized_spacy  \n",
       "0                                                                                                        figlio piccolo insegnato baciare labbro 30 madre affetto ci rimpiango vergognare problema sessualità  \n",
       "1                                                                                                                                          il followers front man Dari informatico 42 Dario cercare é omonimo  \n",
       "2                                                                                                                                                                                    51 capitare provare odio  \n",
       "3                                                                                                                                   ciao Barbara   intervento lemme grasso 53 pronto mettere mi gioco seguire  \n",
       "4                                                                                                          diplomato scuola alberghiero aiuto cuoco Lavapiatti portiere notturno istruttore scuola guidare 42  \n",
       "...                                                                                                                                                                                                       ...  \n",
       "1114                                                                                                                                 chiare Marianna 28   volere credere voto tristemente andrò votare schifo  \n",
       "1115  populismo sinistra populismo destraAggrappare a il unico vero figura serio il attuale presidente credere letto sentire nascere 47 anni):”faccare il Italia tedesco volere preparazione cultura politico  \n",
       "1116                                                                                                                                                                                                      nan  \n",
       "1117                                                                                                                                             49anni volere politico compenso risolvere il problema Italia  \n",
       "1118                                                                                                                                                                                                      nan  \n",
       "\n",
       "[1119 rows x 2 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576356eb-c498-4991-a868-314ebcec993d",
   "metadata": {},
   "source": [
    "STANZAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "479ed9c3-c9d7-4e85-a87d-0d96d1cd6a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6dee8d3-6399-4bfc-a016-0af7a3a45f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fbc38489-6457-41bf-8fc1-443a8b159aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 2,\n",
      "  \"text\": \"StefanoGuerrera\",\n",
      "  \"start_char\": 1,\n",
      "  \"end_char\": 16\n",
      "}\n",
      "{\n",
      "  \"id\": 5,\n",
      "  \"text\": \"figlia\",\n",
      "  \"start_char\": 26,\n",
      "  \"end_char\": 32\n",
      "}\n",
      "{\n",
      "  \"id\": 6,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 32,\n",
      "  \"end_char\": 33\n",
      "}\n",
      "{\n",
      "  \"id\": 8,\n",
      "  \"text\": \"piccola\",\n",
      "  \"start_char\": 37,\n",
      "  \"end_char\": 44\n",
      "}\n",
      "{\n",
      "  \"id\": 12,\n",
      "  \"text\": \"insegnato\",\n",
      "  \"start_char\": 58,\n",
      "  \"end_char\": 67\n",
      "}\n",
      "{\n",
      "  \"id\": 14,\n",
      "  \"text\": \"baciare\",\n",
      "  \"start_char\": 70,\n",
      "  \"end_char\": 77\n",
      "}\n",
      "{\n",
      "  \"id\": 17,\n",
      "  \"text\": \"labbra\",\n",
      "  \"start_char\": 84,\n",
      "  \"end_char\": 90\n",
      "}\n",
      "{\n",
      "  \"id\": 18,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 90,\n",
      "  \"end_char\": 91\n",
      "}\n",
      "{\n",
      "  \"id\": 21,\n",
      "  \"text\": \".\",\n",
      "  \"start_char\": 101,\n",
      "  \"end_char\": 102\n",
      "}\n",
      "{\n",
      "  \"id\": 3,\n",
      "  \"text\": \"30\",\n",
      "  \"start_char\": 110,\n",
      "  \"end_char\": 112\n",
      "}\n",
      "{\n",
      "  \"id\": 4,\n",
      "  \"text\": \"anni\",\n",
      "  \"start_char\": 113,\n",
      "  \"end_char\": 117\n",
      "}\n",
      "{\n",
      "  \"id\": 5,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 117,\n",
      "  \"end_char\": 118\n",
      "}\n",
      "{\n",
      "  \"id\": 10,\n",
      "  \"text\": \"madre\",\n",
      "  \"start_char\": 141,\n",
      "  \"end_char\": 146\n",
      "}\n",
      "{\n",
      "  \"id\": 13,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 156,\n",
      "  \"end_char\": 157\n",
      "}\n",
      "{\n",
      "  \"id\": 15,\n",
      "  \"text\": \"affetto\",\n",
      "  \"start_char\": 162,\n",
      "  \"end_char\": 169\n",
      "}\n",
      "{\n",
      "  \"id\": 16,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 169,\n",
      "  \"end_char\": 170\n",
      "}\n",
      "{\n",
      "  \"id\": 21,\n",
      "  \"text\": \"c’\",\n",
      "  \"start_char\": 186,\n",
      "  \"end_char\": 188\n",
      "}\n",
      "{\n",
      "  \"id\": 25,\n",
      "  \"text\": \"rimpiango\",\n",
      "  \"start_char\": 197,\n",
      "  \"end_char\": 206\n",
      "}\n",
      "{\n",
      "  \"id\": 26,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 206,\n",
      "  \"end_char\": 207\n",
      "}\n",
      "{\n",
      "  \"id\": 28,\n",
      "  \"text\": \"vergognata\",\n",
      "  \"start_char\": 212,\n",
      "  \"end_char\": 222\n",
      "}\n",
      "{\n",
      "  \"id\": 32,\n",
      "  \"text\": \"problemi\",\n",
      "  \"start_char\": 235,\n",
      "  \"end_char\": 243\n",
      "}\n",
      "{\n",
      "  \"id\": 34,\n",
      "  \"text\": \"sessualità\",\n",
      "  \"start_char\": 247,\n",
      "  \"end_char\": 257\n",
      "}\n",
      "{\n",
      "  \"id\": 35,\n",
      "  \"text\": \",\",\n",
      "  \"start_char\": 257,\n",
      "  \"end_char\": 258\n",
      "}\n",
      "{\n",
      "  \"id\": 42,\n",
      "  \"text\": \"...\",\n",
      "  \"start_char\": 287,\n",
      "  \"end_char\": 290\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### just a test code for new tokinizer\n",
    "\n",
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "stop_words_to_remove = ['anni', 'anno']\n",
    "stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "\n",
    "sentence = '@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...'\n",
    "\n",
    "pipeline = stanza.Pipeline('it', processors='tokenize')\n",
    "doc = pipeline(sentence)\n",
    "\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    sentence.words = [\n",
    "        word.text[1:] if word.text.startswith('#') else word\n",
    "        for word in sentence.words if not (\n",
    "            ### different for @ and # since stanza detaches @ as a sepaate token but not #\n",
    "            word.text.startswith('@') or\n",
    "            re.match(r\"http\\S+|www\\S+|https\\S+\", word.text) or\n",
    "            word.text.lower() in stop_words\n",
    "        )\n",
    "    ]\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for tok in sentence.words:\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4215b-245f-45cb-8bcf-ecd3d3bdfa15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "614a0187-4425-45c5-94cc-904afc0aa117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StefanoGuerrera figlia piccola insegnato baciare labbra fatto 30 anni madre facevo affetto c’è rimpiango vergognata problemi sessualità '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### working version of cleaning \n",
    "\n",
    "import string\n",
    "text = tweets[0]\n",
    "words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in text.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "filtered_text = ' '.join(words_new)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cb4bbd4a-ade8-4bea-998d-9b1206e00549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f41356ee-6b78-476f-870c-7b4810f8f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: StefanoGuerrera\n",
      "id: (2,)\ttext: figlia\n",
      "id: (3,)\ttext: piccola\n",
      "id: (4,)\ttext: insegnato\n",
      "id: (5,)\ttext: baciare\n",
      "id: (6,)\ttext: labbra\n",
      "id: (7,)\ttext: fatto\n",
      "id: (8,)\ttext: 30\n",
      "id: (9,)\ttext: anni\n",
      "id: (10,)\ttext: madre\n",
      "id: (11,)\ttext: facevo\n",
      "id: (12,)\ttext: affetto\n",
      "id: (13,)\ttext: c’\n",
      "id: (14,)\ttext: è\n",
      "id: (15,)\ttext: rimpiango\n",
      "id: (16,)\ttext: vergognata\n",
      "id: (17,)\ttext: problemi\n",
      "id: (18,)\ttext: sessualità\n"
     ]
    }
   ],
   "source": [
    "pipeline = stanza.Pipeline('it', processors='tokenize')\n",
    "doc = pipeline(filtered_text)\n",
    "        \n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4fba2f6-6461-4ae1-8a9d-fd9d9b412872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processor for performing tokenization\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "from stanza.models.tokenization.data import TokenizationDataset\n",
    "from stanza.models.tokenization.trainer import Trainer\n",
    "from stanza.models.tokenization.utils import output_predictions\n",
    "from stanza.pipeline._constants import *\n",
    "from stanza.pipeline.processor import UDProcessor, register_processor\n",
    "from stanza.pipeline.registry import PROCESSOR_VARIANTS\n",
    "from stanza.models.common import doc\n",
    "\n",
    "# these imports trigger the \"register_variant\" decorations\n",
    "from stanza.pipeline.external.jieba import JiebaTokenizer\n",
    "from stanza.pipeline.external.spacy import SpacyTokenizer\n",
    "from stanza.pipeline.external.sudachipy import SudachiPyTokenizer\n",
    "from stanza.pipeline.external.pythainlp import PyThaiNLPTokenizer\n",
    "\n",
    "logger = logging.getLogger('stanza')\n",
    "\n",
    "# class for running the tokenizer\n",
    "@register_processor(name= TOKENIZE)\n",
    "class TokenizeProcessor(UDProcessor):\n",
    "\n",
    "    # set of processor requirements this processor fulfills\n",
    "    PROVIDES_DEFAULT = set([TOKENIZE])\n",
    "    # set of processor requirements for this processor\n",
    "    REQUIRES_DEFAULT = set([])\n",
    "    # default max sequence length\n",
    "    MAX_SEQ_LENGTH_DEFAULT = 1000\n",
    "\n",
    "    def _set_up_model(self, config, pipeline, device):\n",
    "        # set up trainer\n",
    "        if config.get('pretokenized'):\n",
    "            self._trainer = None\n",
    "        else:\n",
    "            self._trainer = Trainer(model_file=config['model_path'], device=device)\n",
    "\n",
    "        # get and typecheck the postprocessor\n",
    "        postprocessor = config.get('postprocessor')\n",
    "        if postprocessor and callable(postprocessor):\n",
    "            self._postprocessor = postprocessor\n",
    "        elif not postprocessor:\n",
    "            self._postprocessor = None\n",
    "        else:\n",
    "            raise ValueError(\"Tokenizer recieved 'postprocessor' option of unrecognized type; postprocessor must be callable. Got %s\" % postprocessor)\n",
    "\n",
    "    def process_pre_tokenized_text(self, input_src):\n",
    "        \"\"\"\n",
    "        Pretokenized text can be provided in 2 manners:\n",
    "\n",
    "        1.) str, tokenized by whitespace, sentence split by newline\n",
    "        2.) list of token lists, each token list represents a sentence\n",
    "\n",
    "        generate dictionary data structure\n",
    "        \"\"\"\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in input_src.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        input_src = ' '.join(words_new)\n",
    "        \n",
    "        document = []\n",
    "        \n",
    "        if isinstance(input_src, str):\n",
    "            sentences = [sent.strip().split() for sent in input_src.strip().split('\\n') if len(sent.strip()) > 0]\n",
    "        elif isinstance(input_src, list):\n",
    "            sentences = input_src\n",
    "        idx = 0\n",
    "        for sentence in sentences:\n",
    "            sent = []\n",
    "            for token_id, token in enumerate(sentence):\n",
    "                sent.append({doc.ID: (token_id + 1, ), doc.TEXT: token, doc.MISC: f'start_char={idx}|end_char={idx + len(token)}'})\n",
    "                idx += len(token) + 1\n",
    "            document.append(sent)\n",
    "        raw_text = ' '.join([' '.join(sentence) for sentence in sentences])\n",
    "        return raw_text, document\n",
    "\n",
    "    def process(self, document):\n",
    "        if not (isinstance(document, str) or isinstance(document, doc.Document) or (self.config.get('pretokenized') or self.config.get('no_ssplit', False))):\n",
    "            raise ValueError(\"If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.  Got %s\" % str(type(document)))\n",
    "\n",
    "        if isinstance(document, doc.Document):\n",
    "            if self.config.get('pretokenized'):\n",
    "                return document\n",
    "            document = document.text\n",
    "\n",
    "        if self.config.get('pretokenized'):\n",
    "            raw_text, document = self.process_pre_tokenized_text(document)\n",
    "            return doc.Document(document, raw_text)\n",
    "\n",
    "        if hasattr(self, '_variant'):\n",
    "            return self._variant.process(document)\n",
    "\n",
    "        raw_text = '\\n\\n'.join(document) if isinstance(document, list) else document\n",
    "\n",
    "        max_seq_len = self.config.get('max_seqlen', TokenizeProcessor.MAX_SEQ_LENGTH_DEFAULT)\n",
    "\n",
    "        # set up batches\n",
    "        batches = TokenizationDataset(self.config, input_text=raw_text, vocab=self.vocab, evaluation=True, dictionary=self.trainer.dictionary)\n",
    "        # get dict data\n",
    "        with torch.no_grad():\n",
    "            _, _, _, document = output_predictions(None, self.trainer, batches, self.vocab, None,\n",
    "                                                   max_seq_len,\n",
    "                                                   orig_text=raw_text,\n",
    "                                                   no_ssplit=self.config.get('no_ssplit', False),\n",
    "                                                   num_workers = self.config.get('num_workers', 0),\n",
    "                                                   postprocessor = self._postprocessor)\n",
    "\n",
    "        # replace excessively long tokens with <UNK> to avoid downstream GPU memory issues in POS\n",
    "        for sentence in document:\n",
    "            for token in sentence:\n",
    "                if len(token['text']) > max_seq_len:\n",
    "                    token['text'] = \"<UNK>\"\n",
    "\n",
    "        return doc.Document(document, raw_text)\n",
    "\n",
    "    def bulk_process(self, docs):\n",
    "        \"\"\"\n",
    "        The tokenizer cannot use UDProcessor's sentence-level cross-document batching interface, and requires special handling.\n",
    "        Essentially, this method concatenates the text of multiple documents with \"\\n\\n\", tokenizes it with the neural tokenizer,\n",
    "        then splits the result into the original Documents and recovers the original character offsets.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_variant'):\n",
    "            return self._variant.bulk_process(docs)\n",
    "\n",
    "        if self.config.get('pretokenized'):\n",
    "            res = []\n",
    "            for document in docs:\n",
    "                raw_text, document = self.process_pre_tokenized_text(document.text)\n",
    "                res.append(doc.Document(document, raw_text))\n",
    "            return res\n",
    "\n",
    "        combined_text = '\\n\\n'.join([thisdoc.text for thisdoc in docs])\n",
    "        processed_combined = self.process(doc.Document([], text=combined_text))\n",
    "\n",
    "        # postprocess sentences and tokens to reset back pointers and char offsets\n",
    "        charoffset = 0\n",
    "        sentst = senten = 0\n",
    "        for thisdoc in docs:\n",
    "            while senten < len(processed_combined.sentences) and processed_combined.sentences[senten].tokens[-1].end_char - charoffset <= len(thisdoc.text):\n",
    "                senten += 1\n",
    "\n",
    "            sentences = processed_combined.sentences[sentst:senten]\n",
    "            thisdoc.sentences = sentences\n",
    "            for sent in sentences:\n",
    "                # fix doc back pointers for sentences\n",
    "                sent._doc = thisdoc\n",
    "\n",
    "                # fix char offsets for tokens and words\n",
    "                for token in sent.tokens:\n",
    "                    token._start_char -= charoffset\n",
    "                    token._end_char -= charoffset\n",
    "                    if token.words:  # not-yet-processed MWT can leave empty tokens\n",
    "                        for word in token.words:\n",
    "                            word._start_char -= charoffset\n",
    "                            word._end_char -= charoffset\n",
    "\n",
    "            thisdoc.num_tokens = sum(len(sent.tokens) for sent in sentences)\n",
    "            thisdoc.num_words = sum(len(sent.words) for sent in sentences)\n",
    "            sentst = senten\n",
    "\n",
    "            charoffset += len(thisdoc.text) + 2\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a9b155-9266-40c5-904f-0d43eab64736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 15:02:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473cafbe39cd4a44bf03a81f8133f53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 15:02:59 WARNING: Language it package default expects mwt, which has been added\n",
      "2023-11-20 15:02:59 INFO: Loading these models for language: it (Italian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "| tokenize  | combined |\n",
      "| tokenize  | combined |\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-11-20 15:02:59 INFO: Using device: cpu\n",
      "2023-11-20 15:02:59 INFO: Loading: tokenize\n",
      "2023-11-20 15:02:59 INFO: Loading: mwt\n",
      "2023-11-20 15:02:59 INFO: Loading: tokenize\n",
      "2023-11-20 15:02:59 INFO: Loading: tokenize\n",
      "2023-11-20 15:02:59 INFO: Loading: tokenize\n",
      "2023-11-20 15:02:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='it', processors='TOKENIZE')\n",
    "doc = nlp(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68380f2-08db-4c5d-a7ad-ae7e50c9c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "StefanoGuerrera\n",
      "Sono\n",
      "una\n",
      "figlia\n",
      ",\n",
      "da\n",
      "piccola\n",
      "non\n",
      "mi\n",
      "hanno\n",
      "insegnato\n",
      "a\n",
      "baciare\n",
      "sulle\n",
      "labbra\n",
      ",\n",
      "mai\n",
      "fatto\n",
      ".\n",
      "Ora\n",
      "ho\n",
      "30\n",
      "anni\n",
      ",\n",
      "qualche\n",
      "volta\n",
      "con\n",
      "mia\n",
      "madre\n",
      "lo\n",
      "facevo\n",
      ",\n",
      "per\n",
      "affetto\n",
      ",\n",
      "ed\n",
      "ora\n",
      "che\n",
      "non\n",
      "c’\n",
      "è\n",
      "più\n",
      "lo\n",
      "rimpiango\n",
      ",\n",
      "mai\n",
      "vergognata\n",
      "e\n",
      "mai\n",
      "avuto\n",
      "problemi\n",
      "di\n",
      "sessualità\n",
      ",\n",
      "se\n",
      "ci\n",
      "fosse\n",
      "ancora\n",
      "lo\n",
      "farei\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d10c90e-e643-4636-a43e-28996ee52024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:54:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06a9e2f1a78444eb8b983f78a859a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:54:05 WARNING: Language it package default expects mwt, which has been added\n",
      "2023-11-20 14:54:05 INFO: Loading these models for language: it (Italian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "| tokenize  | combined |\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-11-20 14:54:05 INFO: Using device: cpu\n",
      "2023-11-20 14:54:05 INFO: Loading: tokenize\n",
      "2023-11-20 14:54:05 INFO: Loading: mwt\n",
      "2023-11-20 14:54:05 INFO: Loading: tokenize\n",
      "2023-11-20 14:54:05 INFO: Loading: tokenize\n",
      "2023-11-20 14:54:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "StefanoGuerrera\n",
      "Sono\n",
      "una\n",
      "figlia\n",
      ",\n",
      "da\n",
      "piccola\n",
      "non\n",
      "mi\n",
      "hanno\n",
      "insegnato\n",
      "a\n",
      "baciare\n",
      "sulle\n",
      "labbra\n",
      ",\n",
      "mai\n",
      "fatto\n",
      ".\n",
      "Ora\n",
      "ho\n",
      "30\n",
      "anni\n",
      ",\n",
      "qualche\n",
      "volta\n",
      "con\n",
      "mia\n",
      "madre\n",
      "lo\n",
      "facevo\n",
      ",\n",
      "per\n",
      "affetto\n",
      ",\n",
      "ed\n",
      "ora\n",
      "che\n",
      "non\n",
      "c’\n",
      "è\n",
      "più\n",
      "lo\n",
      "rimpiango\n",
      ",\n",
      "mai\n",
      "vergognata\n",
      "e\n",
      "mai\n",
      "avuto\n",
      "problemi\n",
      "di\n",
      "sessualità\n",
      ",\n",
      "se\n",
      "ci\n",
      "fosse\n",
      "ancora\n",
      "lo\n",
      "farei\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='it', processors='tokenize')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(tweets[0])\n",
    "\n",
    "# Access tokenized sentences and tokens\n",
    "for sentence in doc.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec1374-bcb5-43d9-9eee-9d189f75bffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac507bd4-77a1-4ffe-b204-b7679fb610af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.pipeline.processor import UDProcessor, register_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64925d7a-0be8-477e-a52e-4ebd1e01daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@register_processor(name=TOKENIZE)\n",
    "class TokenizeProcessor(UDProcessor):\n",
    "\n",
    " \n",
    "    PROVIDES_DEFAULT = set([TOKENIZE])\n",
    "    REQUIRES_DEFAULT = set([])\n",
    "    \n",
    "    def process_pre_tokenized_text(self, input_src):\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in input_src.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        filtered_text = ' '.join(words_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ec218d9-0c8e-4d1d-94b3-2b926908cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_processor_variant('Tokenizer', 'Cleaning')\n",
    "class CleaningTokenizer(ProcessorVariant):\n",
    "\n",
    "    OVERRIDE = True\n",
    "\n",
    "    def __init__(self, lang):\n",
    "        pass\n",
    "\n",
    "    def process(self, text):\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "            if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "            for word in text.split()\n",
    "            if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        filtered_text = ' '.join(text_new)\n",
    "        \n",
    "        doc = self.tokenizer_pipeline(filtered_text)\n",
    "        \n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9df1def4-8ca7-4997-8369-33658969797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.pipeline.processor import ProcessorVariant, register_processor_variant\n",
    "from stanza.models.common.doc import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49f254ff-8856-4d42-8f37-2f6885783c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'ladyonorato', 'Ho', '51', 'anni', 'e', 'non', 'mi', 'è', 'mai', 'capitato', 'di', 'provare', 'così', 'tanto', 'odio', 'verso', 'il', 'governo', '!']\n"
     ]
    }
   ],
   "source": [
    "@register_processor_variant('tokenize', 'custom_clean')\n",
    "class CustomTokenizer(ProcessorVariant):\n",
    "    OVERRIDE = True\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer_pipeline = stanza.Pipeline('it', processors='tokenize')\n",
    "\n",
    "    def process(self, text):\n",
    "        doc = self.tokenizer_pipeline(text)\n",
    "        for sentence in doc.sentences:\n",
    "            sentence.words = [\n",
    "                word for word in sentence.words if not (\n",
    "                    re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", word.text) or\n",
    "                    word.text.lower() in stop_words\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return doc\n",
    "\n",
    "# Input sentence\n",
    "sentence = '@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!'\n",
    "\n",
    "# Define stop words\n",
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "stop_words_to_remove = ['anni', 'anno']\n",
    "stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "\n",
    "# Create a Stanza pipeline with the custom tokenizer\n",
    "custom_pipeline = stanza.Pipeline('it', processors={'tokenize': 'custom_clean'})\n",
    "\n",
    "# Process the sentence using the custom pipeline\n",
    "custom_doc = custom_pipeline(sentence)\n",
    "\n",
    "# Access tokenized words in the modified document\n",
    "modified_tokenized_words = [word.text for sentence in custom_doc.sentences for word in sentence.words]\n",
    "print(modified_tokenized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31c26189-5a75-4f86-a186-fed220ff93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ladyonorato None\n",
      "Ho None\n",
      "51 None\n",
      "e None\n",
      "non None\n",
      "mi None\n",
      "è None\n",
      "mai None\n",
      "capitato None\n",
      "di None\n",
      "provare None\n",
      "così None\n",
      "odio None\n",
      "verso None\n",
      "il None\n",
      "governo None\n",
      "! None\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36c0b38a-5880-4d45-b12f-2e606b69be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'ladyonorato', 'Ho', '51', 'anni', 'e', 'non', 'mi', 'è', 'mai', 'capitato', 'di', 'provare', 'così', 'tanto', 'odio', 'verso', 'il', 'governo', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import stanza\n",
    "from stanza.pipeline.processor import ProcessorVariant, register_processor_variant\n",
    "from stanza.models.common.doc import Document\n",
    "\n",
    "# Download the Italian language model\n",
    "stanza.download('it')\n",
    "\n",
    "@register_processor_variant('tokenize', 'custom_clean')\n",
    "class CustomTokenizer(ProcessorVariant):\n",
    "    OVERRIDE = True\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Create a Stanza pipeline for Italian with only the tokenizer\n",
    "        self.tokenizer_pipeline = stanza.Pipeline('it', processors='tokenize')\n",
    "\n",
    "    def process(self, text):\n",
    "        # Process the text using the tokenizer pipeline\n",
    "        doc = self.tokenizer_pipeline(text)\n",
    "\n",
    "        # Define stop words\n",
    "        stop_words = ['anno', 'anni', 'tanto']\n",
    "\n",
    "        # Delete tokens based on specified conditions\n",
    "        for sentence in doc.sentences:\n",
    "            sentence.words = [\n",
    "                word for word in sentence.words if not (\n",
    "                    word.text.startswith('@') or\n",
    "                    word.text.startswith('#') or\n",
    "                    re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", word.text) or\n",
    "                    word.text.lower() in stop_words\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return doc\n",
    "\n",
    "# Input sentence\n",
    "sentence = '@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!'\n",
    "\n",
    "# Create a Stanza pipeline with the custom tokenizer\n",
    "custom_pipeline = stanza.Pipeline('it', processors={'tokenize': 'custom_clean'})\n",
    "\n",
    "# Process the sentence using the custom pipeline\n",
    "custom_doc = custom_pipeline(sentence)\n",
    "\n",
    "# Access tokenized words in the modified document\n",
    "modified_tokenized_words = [word.text for sentence in custom_doc.sentences for word in sentence.words]\n",
    "print(modified_tokenized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "459a645d-7792-4c1d-b87c-92b8858ed9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'ladyonorato', 'Ho', '51', 'anni', 'e', 'non', 'mi', 'è', 'mai', 'capitato', 'di', 'provare', 'così', 'tanto', 'odio', 'verso', 'il', 'governo', '!']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download the Italian language model\n",
    "stanza.download('it')\n",
    "\n",
    "# Create a Stanza pipeline for Italian\n",
    "pipeline = stanza.Pipeline('it', processors='tokenize')\n",
    "\n",
    "# Input sentence\n",
    "sentence = '@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!'\n",
    "\n",
    "# Process the sentence using the pipeline\n",
    "doc = pipeline(sentence)\n",
    "\n",
    "# Access tokenized words\n",
    "tokenized_words = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "print(tokenized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e755de9b-72fa-4500-ab4d-3db308cdc9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'ladyonorato', 'Ho', '51', 'anni', 'e', 'non', 'mi', 'è', 'mai', 'capitato', 'di', 'provare', 'così', 'tanto', 'odio', 'verso', 'il', 'governo', '!']\n"
     ]
    }
   ],
   "source": [
    "custom_pipeline = stanza.Pipeline(lang='it', processors={'tokenize': 'CleaningTokenizing', 'mwt': 'default', 'pos': 'default',\n",
    "                                                         'lemma': 'default'}, verbose=False, use_gpu=False)\n",
    "\n",
    "# Example usage\n",
    "text = '@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!'\n",
    "doc = custom_pipeline(text)\n",
    "\n",
    "# Accessing tokenized words from the custom processor\n",
    "tokenized_words = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02619c52-eb63-4d9c-bd7e-7ba0ea2dcdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ @\n",
      "ladyonorato ladyonorato\n",
      "Ho avere\n",
      "51 51\n",
      "anni anno\n",
      "e e\n",
      "non non\n",
      "mi mi\n",
      "è essere\n",
      "mai mai\n",
      "capitato capitare\n",
      "di di\n",
      "provare provare\n",
      "così così\n",
      "tanto tanto\n",
      "odio odiare\n",
      "verso verso\n",
      "il il\n",
      "governo governo\n",
      "! !\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff715d4-067d-4886-a312-d6725f1c7648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bf677-f6fe-4144-af78-4ef9e4a25bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40644c-df93-43d0-af0f-de4096b6ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp = stanza.Pipeline('it', processors={'tokenize': MyCustomTokenizer, 'mwt': 'default', 'pos': 'default', 'lemma': 'default'}, verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fed79-d55a-4e7d-ae69-0c88c9f7a4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e2a88-9150-4386-b0e6-74db8214c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pipeline with tdqm because it takes a lot of time to process \n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lemmatized_tweets_stanza = []\n",
    "total_start_time = time.time() \n",
    "\n",
    "progress_bar = tqdm(total=len(tweets), desc=\"Processing tweets\", unit=\"tweet\", leave=False)\n",
    "\n",
    "for tweet in tweets_no_hashtags:\n",
    "    doc = it_nlp(tweet)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    lemmatized_tweets_stanza.append(lemmatized_tweet)\n",
    "    \n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time = total_end_time - total_start_time\n",
    "average_time = total_time / len(tweets) if len(tweets) > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal time for all iterations: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {average_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASK 2022.10 (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

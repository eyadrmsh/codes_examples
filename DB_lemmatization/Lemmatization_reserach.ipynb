{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a6ce6c",
   "metadata": {},
   "source": [
    "# Lemmatization reserach\n",
    "In this file you can see codes I created to asess which package is better to use for lemmatization of a db\n",
    "There are two packages that I use: STANZA and Spacy\n",
    "At the end in result of manual check by referees it was decided to not use customized pipilines and to use Spacy instead os `stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import stanza\n",
    "import sqlite3\n",
    "import time\n",
    "import string\n",
    "from spacy.cli import download\n",
    "from spacy.lang.it.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d5fea5-8589-4a84-b9a3-c32c3287a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('it')\n",
    "it_nlp_stanza = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)\n",
    "\n",
    "try:\n",
    "    it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "except OSError:\n",
    "    download('it_core_news_lg')\n",
    "    it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b3047d-5905-4ba6-bdac-545e0be3e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bca768",
   "metadata": {},
   "source": [
    "# Defining function for downloading, lemmatization, cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48d0dee0-f3d8-4733-ae9f-c88b0ce6788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(batch_size):\n",
    "    start_time = time.time()\n",
    "    your_query = f\"SELECT tweet_id, text, language FROM tweets limit {batch_size}\"\n",
    "    df = pd.read_sql_query(your_query, conn)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTime taken to download batch of size {batch_size}: {elapsed_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def itallian_df(df):\n",
    "    start_time = time.time()\n",
    "    itallian_df = df[df['language'] == 'it']\n",
    "    print(f\"\\nTime taken for 'itallian_df': {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"\\nLength of downloaded df {len(df)}\")\n",
    "    return itallian_df\n",
    "\n",
    "def cleaning_df(df):\n",
    "    start_time = time.time()\n",
    "    def cleaning(text):\n",
    "        \n",
    "        stop_words = STOP_WORDS\n",
    "        stop_words_to_remove = ['anni', 'anno']\n",
    "        stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "        if pd.isna(text):\n",
    "                return \"\"\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "        if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        for word in text.split()\n",
    "        if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        filtered_text = ' '.join(words_new)\n",
    "        return filtered_text\n",
    "\n",
    "    df['text'] = df['text'].apply(cleaning)\n",
    "    print(f\"\\nTime taken for 'cleaning_df': {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatizationSpacy_df(df, nlp_model=it_nlp_spacy):\n",
    "    start_time = time.time()\n",
    "    def lemmatizationSpacy(text):\n",
    "        tokens = nlp_model(text)\n",
    "        lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "        return lemmatized_tweet\n",
    "    df['text'] = df['text'].apply(lemmatizationSpacy)\n",
    "    print(f\"\\nTime taken for 'spacy': {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatizationStanza_df(df):\n",
    "    start_time = time.time()\n",
    "    def lemmatizationStanza(text):\n",
    "        doc = it_nlp_stanza(text)\n",
    "        lemmatized_text = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "        return lemmatized_text\n",
    "    df['text'] = df['text'].apply(lemmatizationStanza)\n",
    "    print(f\"\\nTime taken for 'Stanza': {time.time() - start_time:.2f} seconds\")\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def save_to_csv(df, output_file='output_file.csv'):\n",
    "    start_time = time.time()\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDataFrame saved to {output_file}\")\n",
    "    print(f\"\\nTime taken for 'saving': {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d10f5288-486b-4640-b85f-6bf9147964e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken to download batch of size 10000: 0.48 seconds\n"
     ]
    }
   ],
   "source": [
    "batch = download_batch(10000)\n",
    "ital = itallian_df(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed6a851f-1900-42d4-99e0-b53bb6766179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for 'itallian_df': 0.23 seconds\n",
      "\n",
      "Length of downloaded df 10000\n"
     ]
    }
   ],
   "source": [
    "ital = df.drop('language', axis =1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "631cb383-0918-430e-bb64-9b797c33c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for 'Stanza': 701.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/slurm_job.11639884/ipykernel_7762/2199210308.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lemmatizationStanza)\n"
     ]
    }
   ],
   "source": [
    "df = lemmatizationStanza_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0c4d6",
   "metadata": {},
   "source": [
    "# Building pipilines components for spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef0a1d-52c3-4ae7-bd86-285f904dfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_lg')\n",
    "\n",
    "### for future how to display whole tweets \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "components_to_remove = ['tagger', 'parser', 'attribute_ruler', 'ner']\n",
    "for component in components_to_remove:\n",
    "    nlp.remove_pipe(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5913c7f2-f2c4-4bf3-b0ab-dde08ec64654",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding cleaning component\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"cleaning\")\n",
    "def cleaning(doc):\n",
    "    stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "    new_tokens = [token.text for token in doc if not re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", token.text) and token.text.lower() not in stop_words]\n",
    "    return spacy.tokens.Doc(doc.vocab, words=new_tokens)\n",
    "\n",
    "nlp.add_pipe(\"cleaning\", name=\"cleaning\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "lemmatized_tweets_spacy = []\n",
    "for tweet in tweets:\n",
    "    tokens = nlp(tweet)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    lemmatized_tweets_spacy.append(lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda61922-f7b5-492a-9a7c-673031c12caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e988559a9c94495ba0dd9ea468cfc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 15:32:08 INFO: Downloading default packages for language: it (Italian) ...\n",
      "2023-11-13 15:32:09 INFO: File exists: /g100/home/userexternal/ddurmush/stanza_resources/it/default.zip\n",
      "2023-11-13 15:32:13 INFO: Finished downloading models and saved to /g100/home/userexternal/ddurmush/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0eb75b3-d1db-4f53-ab9d-b15719d934d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be064f2-fb77-47a2-adf7-2c899a443e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets just clean it here \n",
    "## stanza does not have stop words for italian\n",
    "\n",
    "tweets_no_links_mentions = [re.sub(r\"(http\\S+|www\\S+|https\\S+|@\\S+)\", \"\", tweet, flags=re.MULTILINE) for tweet in tweets]\n",
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "\n",
    "tweets_no_links_mentions_stopwords = []\n",
    "\n",
    "for tweet in tweets_no_links_mentions:\n",
    "    filtered_tweet = ' '.join(word for word in tweet.split() if word.lower() not in stop_words)\n",
    "    tweets_no_links_mentions_stopwords.append(filtered_tweet)\n",
    "    \n",
    "tweets_no_hashtags = [\n",
    "    re.sub(r'#', '', tweet)\n",
    "    for tweet in tweets_no_links_mentions_stopwords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding a costumised stage of pipiline for deleting #,@ and stopwords\n",
    "\n",
    "class RemoveTweetsMentionsStopwordsProcessor:\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        self.stop_words = {'stopword1', 'stopword2', 'stopword3'}  \n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        tweets_mentions_stopwords_removed = []\n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            cleaned_tokens = []\n",
    "            for tok in sent.tokens:\n",
    "\n",
    "                if (\n",
    "                    not tok.text.startswith('@')\n",
    "                    or not tok.text.startswith('#')\n",
    "                    or tok.text.lower() not in self.stop_words\n",
    "                ):\n",
    "                    cleaned_tokens.append(tok.text)\n",
    "\n",
    "            cleaned_sentence = ' '.join(cleaned_tokens)\n",
    "            tweets_mentions_stopwords_removed.append(cleaned_sentence)\n",
    "\n",
    "        doc.text = ' '.join(tweets_mentions_stopwords_removed)\n",
    "\n",
    "        return doc\n",
    "\n",
    "stanza.download('it')\n",
    "\n",
    "custom_pipeline = stanza.Pipeline('it', processors={'tokenize': 'it', 'remove_tweets_mentions_stopwords': RemoveTweetsMentionsStopwordsProcessor})\n",
    "\n",
    "text = \"This is a @mention and #hashtag example.\"\n",
    "doc = custom_pipeline(text)\n",
    "\n",
    "cleaned_text = doc.text\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c98cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.pipeline.processor import ProcessorVariant, register_processor_variant\n",
    "from stanza.pipeline.processor import Processor\n",
    "from stanza.pipeline.core import register_processor\n",
    "\n",
    "@register_processor(\"remove_tweets_mentions\")\n",
    "class RemoveTweetsMentionsProcessor(Processor):\n",
    "    ''' Processor that cleanes '''\n",
    "    _requires = set(['tokenize'])\n",
    "    _provides = set(['cleaned'])\n",
    "\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        tweets_mentions_removed = []\n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            cleaned_tokens = []\n",
    "            for tok in sent.tokens:\n",
    "                if (not tok.text.startswith('@')\n",
    "                    or not tok.text.startswith('#')\n",
    "                    or not re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", tok.text)\n",
    "                    or tok.text.lower() not in stop_words):\n",
    "                    cleaned_tokens.append(tok.text)\n",
    "\n",
    "            cleaned_sentence = ' '.join(cleaned_tokens)\n",
    "            tweets_mentions_removed.append(cleaned_sentence)\n",
    "\n",
    "        doc.text = ' '.join(tweets_mentions_removed)\n",
    "        \n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdf8f6-cf1a-4da8-b7b0-53378e64542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_processor(\"cleaning\")\n",
    "class LowercaseProcessor(Processor):\n",
    "    ''' Processor that lowercases all text '''\n",
    "    _requires = set(['tokenize'])\n",
    "    _provides = set(['lowercase'])\n",
    "\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        doc.text = doc.text.lower()\n",
    "        for sent in doc.sentences:\n",
    "            for tok in sent.tokens:\n",
    "                tok.text = tok.text.lower()\n",
    "\n",
    "            for word in sent.words:\n",
    "                word.text = word.text.lower()\n",
    "                \n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2810f-98cd-4191-986d-25d001b68df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors='tokenize,lowercase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1224a608-ef90-4d70-85ce-00d1b1225bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tweets:   0%|          | 0/1119 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for all iterations: 167.74 seconds\n",
      "Average time per item: 0.1499 seconds\n"
     ]
    }
   ],
   "source": [
    "### pipeline with tdqm because it takes a lot of time to process \n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lemmatized_tweets_stanza = []\n",
    "total_start_time = time.time() \n",
    "\n",
    "progress_bar = tqdm(total=len(tweets), desc=\"Processing tweets\", unit=\"tweet\", leave=False)\n",
    "\n",
    "for tweet in tweets_no_hashtags:\n",
    "    doc = it_nlp(tweet)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    lemmatized_tweets_stanza.append(lemmatized_tweet)\n",
    "    \n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time = total_end_time - total_start_time\n",
    "average_time = total_time / len(tweets) if len(tweets) > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal time for all iterations: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {average_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26984dd3-3f95-4251-8d9d-a396faa3837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check = pd.DataFrame({'original': tweets, 'lemmatized_spacy':lemmatized_tweets_spacy, 'lemmatized_stanza':lemmatized_tweets_stanza  })\n",
    "df_to_check.to_csv('lemmas_spacy_stanza.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

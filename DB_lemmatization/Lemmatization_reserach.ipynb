{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5088ee85-6ead-49c1-b78f-be8165d90d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import stanza\n",
    "import sqlite3\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f521a97-ab2a-4d51-9ac6-073cbb3e584f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d5fea5-8589-4a84-b9a3-c32c3287a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('it')\n",
    "it_nlp_stanza = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)\n",
    "\n",
    "try:\n",
    "    it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "except OSError:\n",
    "    download('it_core_news_lg')\n",
    "    it_nlp_spacy = spacy.load('it_core_news_lg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b3047d-5905-4ba6-bdac-545e0be3e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d19cfcb0-6df7-4881-b99d-5e4016401f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.cli import download\n",
    "from spacy.lang.it.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48d0dee0-f3d8-4733-ae9f-c88b0ce6788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(batch_size):\n",
    "    start_time = time.time()\n",
    "    your_query = f\"SELECT tweet_id, text, language FROM tweets limit {batch_size}\"\n",
    "    df = pd.read_sql_query(your_query, conn)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTime taken to download batch of size {batch_size}: {elapsed_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def itallian_df(df):\n",
    "    start_time = time.time()\n",
    "    itallian_df = df[df['language'] == 'it']\n",
    "    print(f\"\\nTime taken for 'itallian_df': {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"\\nLength of downloaded df {len(df)}\")\n",
    "    return itallian_df\n",
    "\n",
    "def cleaning_df(df):\n",
    "    start_time = time.time()\n",
    "    def cleaning(text):\n",
    "        \n",
    "        stop_words = STOP_WORDS\n",
    "        stop_words_to_remove = ['anni', 'anno']\n",
    "        stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "        if pd.isna(text):\n",
    "                return \"\"\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "        if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        for word in text.split()\n",
    "        if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        filtered_text = ' '.join(words_new)\n",
    "        return filtered_text\n",
    "\n",
    "    df['text'] = df['text'].apply(cleaning)\n",
    "    print(f\"\\nTime taken for 'cleaning_df': {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatizationSpacy_df(df, nlp_model=it_nlp_spacy):\n",
    "    start_time = time.time()\n",
    "    def lemmatizationSpacy(text):\n",
    "        tokens = nlp_model(text)\n",
    "        lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "        return lemmatized_tweet\n",
    "    df['text'] = df['text'].apply(lemmatizationSpacy)\n",
    "    print(f\"\\nTime taken for 'spacy': {time.time() - start_time:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatizationStanza_df(df):\n",
    "    start_time = time.time()\n",
    "    def lemmatizationStanza(text):\n",
    "        doc = it_nlp_stanza(text)\n",
    "        lemmatized_text = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "        return lemmatized_text\n",
    "    df['text'] = df['text'].apply(lemmatizationStanza)\n",
    "    print(f\"\\nTime taken for 'Stanza': {time.time() - start_time:.2f} seconds\")\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def save_to_csv(df, output_file='output_file.csv'):\n",
    "    start_time = time.time()\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDataFrame saved to {output_file}\")\n",
    "    print(f\"\\nTime taken for 'saving': {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d10f5288-486b-4640-b85f-6bf9147964e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken to download batch of size 10000: 0.48 seconds\n"
     ]
    }
   ],
   "source": [
    "batch = download_batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed6a851f-1900-42d4-99e0-b53bb6766179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for 'itallian_df': 0.23 seconds\n",
      "\n",
      "Length of downloaded df 10000\n"
     ]
    }
   ],
   "source": [
    "ital = itallian_df(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "801c55ba-ef2c-45e4-9735-2702fb275023",
   "metadata": {},
   "outputs": [],
   "source": [
    "ital = df.drop('language', axis =1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c400c61f-221a-428c-8ae6-0d04d60475fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002833</td>\n",
       "      <td>preparare accomparire mamma regalo natale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2002923</td>\n",
       "      <td>cercare capire funzionare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2054033</td>\n",
       "      <td>colpa beggio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2057643</td>\n",
       "      <td>cercare capire funzionare twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2082733</td>\n",
       "      <td>tagliare capello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>173390792</td>\n",
       "      <td>motokrzr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>173443782</td>\n",
       "      <td>rosso spellare peperona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>173610562</td>\n",
       "      <td>luna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>174264072</td>\n",
       "      <td>buondì twitters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>174347682</td>\n",
       "      <td>computer accendere libro tastiera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                       text\n",
       "8       1002833  preparare accomparire mamma regalo natale\n",
       "39      2002923                  cercare capire funzionare\n",
       "65      2054033                               colpa beggio\n",
       "67      2057643          cercare capire funzionare twitter\n",
       "91      2082733                           tagliare capello\n",
       "...         ...                                        ...\n",
       "9987  173390792                                   motokrzr\n",
       "9989  173443782                    rosso spellare peperona\n",
       "9991  173610562                                       luna\n",
       "9996  174264072                            buondì twitters\n",
       "9999  174347682          computer accendere libro tastiera\n",
       "\n",
       "[5766 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e85614e-7495-43a7-bf18-6f93630c4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def cleaning(text):\n",
    "        \n",
    "        stop_words = STOP_WORDS\n",
    "        stop_words_to_remove = ['anni', 'anno']\n",
    "        stop_words = [word for word in stop_words if word not in stop_words_to_remove]\n",
    "        if pd.isna(text):\n",
    "                return \"\"\n",
    "        words_new = [word[1:].translate(str.maketrans('', '', string.punctuation)) \n",
    "        if (word.startswith('#') or word.startswith('@')) else word.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        for word in text.split()\n",
    "        if not (re.match(r\"http\\S+|www\\S+|https\\S+\", word) or word.lower() in stop_words)]\n",
    "        filtered_text = ' '.join(words_new)\n",
    "        return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8545160f-8df9-4a50-8cdd-4f6d65f6bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi preparo e poi accompagno mamma a fare regali di natale\n"
     ]
    }
   ],
   "source": [
    "print(ital['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bf2948c-99b0-4d80-a8fa-f460251f6df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preparo accompagno mamma regali natale'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning(ital['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0451c10-ad19-4df2-9ad9-b4ae7a88ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for 'cleaning_df': 0.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/slurm_job.11639884/ipykernel_7762/2167672894.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(cleaning)\n"
     ]
    }
   ],
   "source": [
    "clean = cleaning_df(ital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1ea3737-617c-49be-bf54-0c137d2637c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002833</td>\n",
       "      <td>mi preparo e poi accompagno mamma a fare regal...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2002923</td>\n",
       "      <td>sto cercando di capire come funziona!!!!!!!</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2054033</td>\n",
       "      <td>Tutta colpa di Beggi!!!</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2057643</td>\n",
       "      <td>cerco di capire come funziona questo twitter\\n</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2082733</td>\n",
       "      <td>Ho tagliato i capelli (tutti)</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>173390792</td>\n",
       "      <td>@tutti qualcuno possiede un motokrzr?</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>173443782</td>\n",
       "      <td>è rossa spellata, è come un peperone.</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>173610562</td>\n",
       "      <td>È quasi pieno, come la luna</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>174264072</td>\n",
       "      <td>BuonDì Twitters</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>174347682</td>\n",
       "      <td>ha il computer acceso, ma un libro sulla tasti...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                               text language\n",
       "8       1002833  mi preparo e poi accompagno mamma a fare regal...       it\n",
       "39      2002923        sto cercando di capire come funziona!!!!!!!       it\n",
       "65      2054033                            Tutta colpa di Beggi!!!       it\n",
       "67      2057643     cerco di capire come funziona questo twitter\\n       it\n",
       "91      2082733                      Ho tagliato i capelli (tutti)       it\n",
       "...         ...                                                ...      ...\n",
       "9987  173390792              @tutti qualcuno possiede un motokrzr?       it\n",
       "9989  173443782              è rossa spellata, è come un peperone.       it\n",
       "9991  173610562                        È quasi pieno, come la luna       it\n",
       "9996  174264072                                    BuonDì Twitters       it\n",
       "9999  174347682  ha il computer acceso, ma un libro sulla tasti...       it\n",
       "\n",
       "[5766 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30782551-86d1-4502-8aca-2cd8083517bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002833</td>\n",
       "      <td>preparo accompagno mamma regali natale</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2002923</td>\n",
       "      <td>cercando capire funziona</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2054033</td>\n",
       "      <td>colpa beggi</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2057643</td>\n",
       "      <td>cerco capire funziona twitter</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2082733</td>\n",
       "      <td>tagliato i capelli tutti</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>173390792</td>\n",
       "      <td>tutti possiede motokrzr</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>173443782</td>\n",
       "      <td>rossa spellata peperone</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>173610562</td>\n",
       "      <td>pieno luna</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>174264072</td>\n",
       "      <td>buondì twitters</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>174347682</td>\n",
       "      <td>computer acceso libro tastiera</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                    text language\n",
       "8       1002833  preparo accompagno mamma regali natale       it\n",
       "39      2002923                cercando capire funziona       it\n",
       "65      2054033                             colpa beggi       it\n",
       "67      2057643           cerco capire funziona twitter       it\n",
       "91      2082733                tagliato i capelli tutti       it\n",
       "...         ...                                     ...      ...\n",
       "9987  173390792                 tutti possiede motokrzr       it\n",
       "9989  173443782                 rossa spellata peperone       it\n",
       "9991  173610562                              pieno luna       it\n",
       "9996  174264072                         buondì twitters       it\n",
       "9999  174347682          computer acceso libro tastiera       it\n",
       "\n",
       "[5766 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfdf9b94-35bb-4424-b69a-9c1c8088dd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8       True\n",
       "39      True\n",
       "65      True\n",
       "67      True\n",
       "91      True\n",
       "        ... \n",
       "9987    True\n",
       "9989    True\n",
       "9991    True\n",
       "9996    True\n",
       "9999    True\n",
       "Name: text, Length: 5766, dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ital['text'] == clean['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9539701-d7b8-4ec7-96ec-199ac4c3befd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002833</td>\n",
       "      <td>preparare accomparire mamma regalo natale</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2002923</td>\n",
       "      <td>cercare capire funzionare</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2054033</td>\n",
       "      <td>colpa beggio</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2057643</td>\n",
       "      <td>cercare capire funzionare twitter</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2082733</td>\n",
       "      <td>tagliare capello</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>173390792</td>\n",
       "      <td>motokrzr</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>173443782</td>\n",
       "      <td>rosso spellare peperona</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>173610562</td>\n",
       "      <td>luna</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>174264072</td>\n",
       "      <td>buondì twitters</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>174347682</td>\n",
       "      <td>computer accendere libro tastiera</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                       text language\n",
       "8       1002833  preparare accomparire mamma regalo natale       it\n",
       "39      2002923                  cercare capire funzionare       it\n",
       "65      2054033                               colpa beggio       it\n",
       "67      2057643          cercare capire funzionare twitter       it\n",
       "91      2082733                           tagliare capello       it\n",
       "...         ...                                        ...      ...\n",
       "9987  173390792                                   motokrzr       it\n",
       "9989  173443782                    rosso spellare peperona       it\n",
       "9991  173610562                                       luna       it\n",
       "9996  174264072                            buondì twitters       it\n",
       "9999  174347682          computer accendere libro tastiera       it\n",
       "\n",
       "[5766 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "631cb383-0918-430e-bb64-9b797c33c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for 'Stanza': 701.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/slurm_job.11639884/ipykernel_7762/2199210308.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lemmatizationStanza)\n"
     ]
    }
   ],
   "source": [
    "df = lemmatizationStanza_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dbb3c75-220b-48a8-bd4d-71400fed7bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8       True\n",
       "39      True\n",
       "65      True\n",
       "67      True\n",
       "91      True\n",
       "        ... \n",
       "9987    True\n",
       "9989    True\n",
       "9991    True\n",
       "9996    True\n",
       "9999    True\n",
       "Name: text, Length: 5766, dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] == dff['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb444333-8285-4480-828f-534e122ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/g100_work/IscrC_mental/data/user_classification/user_age_gender_location_test_set.pkl', 'rb') as file:\n",
    "    data_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a08aae7-7f64-4f82-be38-b72fb28abdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(data_test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebc659-520f-4339-91cf-976d9cc44264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8389cd1-084b-486f-9edb-b54c3f5c72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [str(tw) for tw in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9ad72-e3f0-414d-8050-6336d7ea06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ef0a1d-52c3-4ae7-bd86-285f904dfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8afbfbd-360f-40ce-b18c-c0ffbfcf42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for future how to display whole tweets \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f6ff88-b9df-4915-9929-8bb7fbc47ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_to_remove = ['tagger', 'parser', 'attribute_ruler', 'ner']\n",
    "for component in components_to_remove:\n",
    "    nlp.remove_pipe(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5913c7f2-f2c4-4bf3-b0ab-dde08ec64654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"cleaning\")\n",
    "def cleaning(doc):\n",
    "    stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "    new_tokens = [token.text for token in doc if not re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", token.text) and token.text.lower() not in stop_words]\n",
    "    return spacy.tokens.Doc(doc.vocab, words=new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6fcfa98-7c8c-48c5-9d67-04e21758ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaning', 'tok2vec', 'morphologizer', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"cleaning\", name=\"cleaning\", first=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10524516-d1d5-4fe1-bac4-75f087ce4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### trying with different modes - doesn't work\n",
    "\n",
    "nlp.remove_pipe('lemmatizer')\n",
    "\n",
    "config = {\"mode\": \"rule\", \"overwrite\": True}\n",
    "lemmatizer = nlp.add_pipe(\"lemmatizer\", config=config)\n",
    "\n",
    "doc = nlp(tweets[0])\n",
    "for token in doc:\n",
    "    print(f'pos: {token.pos_}, text: {token.text}, lemma: {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51438640-dd97-47b2-81f6-a6b820e45827",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tweets_spacy = []\n",
    "for tweet in tweets:\n",
    "    tokens = nlp(tweet)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    lemmatized_tweets_spacy.append(lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b314e6-881d-4f91-a8d3-76620431d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tweets:   0%|          | 0/1119 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for all iterations: 4.96 seconds\n",
      "Average time per item: 0.0044 seconds\n"
     ]
    }
   ],
   "source": [
    "### the same but with time analysis \n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lemmatized_tweets_spacy = []\n",
    "total_start_time = time.time() \n",
    "\n",
    "\n",
    "progress_bar = tqdm(total=len(tweets), desc=\"Processing tweets\", unit=\"tweet\", leave=False)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tokens = nlp(tweet)\n",
    "    lemmatized_tweet = \" \".join([token.lemma_ for token in tokens])\n",
    "    lemmatized_tweets_spacy.append(lemmatized_tweet)\n",
    "    \n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "total_end_time = time.time() \n",
    "total_time = total_end_time - total_start_time\n",
    "average_time = total_time / len(tweets) if len(tweets) > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal time for all iterations: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {average_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48106a00-297e-42b2-bfc3-1b91e99c82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### trying stanza!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d79e611-d0da-4c22-8645-5ccdde71faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda61922-f7b5-492a-9a7c-673031c12caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e988559a9c94495ba0dd9ea468cfc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 15:32:08 INFO: Downloading default packages for language: it (Italian) ...\n",
      "2023-11-13 15:32:09 INFO: File exists: /g100/home/userexternal/ddurmush/stanza_resources/it/default.zip\n",
      "2023-11-13 15:32:13 INFO: Finished downloading models and saved to /g100/home/userexternal/ddurmush/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0eb75b3-d1db-4f53-ab9d-b15719d934d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be064f2-fb77-47a2-adf7-2c899a443e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets just clean it here \n",
    "## stanza does not have stop words for italian\n",
    "\n",
    "tweets_no_links_mentions = [re.sub(r\"(http\\S+|www\\S+|https\\S+|@\\S+)\", \"\", tweet, flags=re.MULTILINE) for tweet in tweets]\n",
    "stop_words = spacy.lang.it.stop_words.STOP_WORDS\n",
    "\n",
    "tweets_no_links_mentions_stopwords = []\n",
    "\n",
    "for tweet in tweets_no_links_mentions:\n",
    "    filtered_tweet = ' '.join(word for word in tweet.split() if word.lower() not in stop_words)\n",
    "    tweets_no_links_mentions_stopwords.append(filtered_tweet)\n",
    "    \n",
    "tweets_no_hashtags = [\n",
    "    re.sub(r'#', '', tweet)\n",
    "    for tweet in tweets_no_links_mentions_stopwords\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf821abf-7688-43f5-ab3c-870c76651252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b12979461c843289b7c50ec6679858b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 19:42:20 INFO: Downloading default packages for language: it (Italian) ...\n",
      "2023-11-16 19:42:21 INFO: File exists: /g100/home/userexternal/ddurmush/stanza_resources/it/default.zip\n",
      "2023-11-16 19:42:24 INFO: Finished downloading models and saved to /g100/home/userexternal/ddurmush/stanza_resources.\n",
      "2023-11-16 19:42:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6102436a67a246f79c2d1afb741ef48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'RemoveTweetsMentionsStopwordsProcessor' has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create the pipeline with the custom processor\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m custom_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mremove_tweets_mentions_stopwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRemoveTweetsMentionsStopwordsProcessor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Process text\u001b[39;00m\n\u001b[1;32m     39\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a @mention and #hashtag example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:220\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     download_resources_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir,\n\u001b[1;32m    213\u001b[0m                             resources_url\u001b[38;5;241m=\u001b[39mresources_url,\n\u001b[1;32m    214\u001b[0m                             resources_branch\u001b[38;5;241m=\u001b[39mresources_branch,\n\u001b[1;32m    215\u001b[0m                             resources_version\u001b[38;5;241m=\u001b[39mresources_version,\n\u001b[1;32m    216\u001b[0m                             resources_filepath\u001b[38;5;241m=\u001b[39mresources_filepath,\n\u001b[1;32m    217\u001b[0m                             proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# process different pipeline parameters\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m lang, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir, package, processors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pipeline_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Load resources.json to obtain latest packages.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading resource file...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/resources/common.py:405\u001b[0m, in \u001b[0;36mprocess_pipeline_parameters\u001b[0;34m(lang, model_dir, package, processors)\u001b[0m\n\u001b[1;32m    403\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processors, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 405\u001b[0m     processors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m         k\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower(): ([v_i\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m v_i \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m processors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be dict or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(processors)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/resources/common.py:406\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    403\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processors, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    405\u001b[0m     processors \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 406\u001b[0m         k\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower(): ([v_i\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m v_i \u001b[38;5;129;01min\u001b[39;00m v] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m processors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be dict or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(processors)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'RemoveTweetsMentionsStopwordsProcessor' has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "class RemoveTweetsMentionsStopwordsProcessor:\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        self.stop_words = {'stopword1', 'stopword2', 'stopword3'}  # Add your stop words here\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        tweets_mentions_stopwords_removed = []\n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            cleaned_tokens = []\n",
    "            for tok in sent.tokens:\n",
    "                # Check if the token is not a mention, hashtag, or in the stop words\n",
    "                if (\n",
    "                    not tok.text.startswith('@')\n",
    "                    or not tok.text.startswith('#')\n",
    "                    or tok.text.lower() not in self.stop_words\n",
    "                ):\n",
    "                    cleaned_tokens.append(tok.text)\n",
    "\n",
    "            cleaned_sentence = ' '.join(cleaned_tokens)\n",
    "            tweets_mentions_stopwords_removed.append(cleaned_sentence)\n",
    "\n",
    "        # Update the document with the cleaned text\n",
    "        doc.text = ' '.join(tweets_mentions_stopwords_removed)\n",
    "\n",
    "        return doc\n",
    "\n",
    "# Download the Italian models\n",
    "stanza.download('it')\n",
    "\n",
    "# Create the pipeline with the custom processor\n",
    "custom_pipeline = stanza.Pipeline('it', processors={'tokenize': 'it', 'remove_tweets_mentions_stopwords': RemoveTweetsMentionsStopwordsProcessor})\n",
    "\n",
    "# Process text\n",
    "text = \"This is a @mention and #hashtag example.\"\n",
    "doc = custom_pipeline(text)\n",
    "\n",
    "# Access the cleaned text\n",
    "cleaned_text = doc.text\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f8673b9-0ed8-4740-a359-8d046b560b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.pipeline.processor import ProcessorVariant, register_processor_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41dae4f-7a27-43eb-be2c-96f538f095e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'register_processor' from 'stanza.pipeline.core' (/g100/home/userexternal/ddurmush/.local/lib/python3.10/site-packages/stanza/pipeline/core.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Processor\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_processor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@register_processor\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_tweets_mentions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRemoveTweetsMentionsProcessor\u001b[39;00m(Processor):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Processor that cleanes '''\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'register_processor' from 'stanza.pipeline.core' (/g100/home/userexternal/ddurmush/.local/lib/python3.10/site-packages/stanza/pipeline/core.py)"
     ]
    }
   ],
   "source": [
    "from stanza.pipeline.processor import Processor\n",
    "from stanza.pipeline.core import register_processor\n",
    "\n",
    "@register_processor(\"remove_tweets_mentions\")\n",
    "class RemoveTweetsMentionsProcessor(Processor):\n",
    "    ''' Processor that cleanes '''\n",
    "    _requires = set(['tokenize'])\n",
    "    _provides = set(['cleaned'])\n",
    "\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        tweets_mentions_removed = []\n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            cleaned_tokens = []\n",
    "            for tok in sent.tokens:\n",
    "                if (not tok.text.startswith('@')\n",
    "                    or not tok.text.startswith('#')\n",
    "                    or not re.match(r\"http\\S+|www\\S+|https\\S+|@\\S+|#(?!\\w)\", tok.text)\n",
    "                    or tok.text.lower() not in stop_words):\n",
    "                    cleaned_tokens.append(tok.text)\n",
    "\n",
    "            cleaned_sentence = ' '.join(cleaned_tokens)\n",
    "            tweets_mentions_removed.append(cleaned_sentence)\n",
    "\n",
    "        doc.text = ' '.join(tweets_mentions_removed)\n",
    "        \n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdf8f6-cf1a-4da8-b7b0-53378e64542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_processor(\"cleaning\")\n",
    "class LowercaseProcessor(Processor):\n",
    "    ''' Processor that lowercases all text '''\n",
    "    _requires = set(['tokenize'])\n",
    "    _provides = set(['lowercase'])\n",
    "\n",
    "    def __init__(self, device, config, pipeline):\n",
    "        pass\n",
    "\n",
    "    def _set_up_model(self, *args):\n",
    "        pass\n",
    "\n",
    "    def process(self, doc):\n",
    "        doc.text = doc.text.lower()\n",
    "        for sent in doc.sentences:\n",
    "            for tok in sent.tokens:\n",
    "                tok.text = tok.text.lower()\n",
    "\n",
    "            for word in sent.words:\n",
    "                word.text = word.text.lower()\n",
    "\n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2810f-98cd-4191-986d-25d001b68df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(dir=TEST_MODELS_DIR, lang='en', processors='tokenize,lowercase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12dfce-c4de-4382-88f5-346f15a973a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813f60f-a800-44d9-84df-d6251f46d3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8c8e4-840a-45d9-9b48-e5e70cd0a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### let's run pipeline \n",
    "\n",
    "lemmatized_tweets_stanza = []\n",
    "for tweet in tweets_no_hashtags :\n",
    "    doc = it_nlp(tweet)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    lemmatized_tweets_stanza.append(lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1224a608-ef90-4d70-85ce-00d1b1225bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tweets:   0%|          | 0/1119 [00:00<?, ?tweet/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time for all iterations: 167.74 seconds\n",
      "Average time per item: 0.1499 seconds\n"
     ]
    }
   ],
   "source": [
    "### pipeline with tdqm because it takes a lot of time to process \n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lemmatized_tweets_stanza = []\n",
    "total_start_time = time.time() \n",
    "\n",
    "progress_bar = tqdm(total=len(tweets), desc=\"Processing tweets\", unit=\"tweet\", leave=False)\n",
    "\n",
    "for tweet in tweets_no_hashtags:\n",
    "    doc = it_nlp(tweet)\n",
    "    lemmatized_tweet = ' '.join([word.lemma if word.lemma.endswith((',', '.')) else word.lemma + ' ' for sent in doc.sentences for word in sent.words])\n",
    "    lemmatized_tweets_stanza.append(lemmatized_tweet)\n",
    "    \n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_time = total_end_time - total_start_time\n",
    "average_time = total_time / len(tweets) if len(tweets) > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal time for all iterations: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per item: {average_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26984dd3-3f95-4251-8d9d-a396faa3837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_check = pd.DataFrame({'original': tweets, 'lemmatized_spacy':lemmatized_tweets_spacy, 'lemmatized_stanza':lemmatized_tweets_stanza  })\n",
    "df_to_check.to_csv('lemmas_spacy_stanza.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986edc86-0293-4698-b752-56e59e5b2b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemmatized_spacy</th>\n",
       "      <th>lemmatized_stanza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...</td>\n",
       "      <td>figlio , piccolo insegnato baciare labbro , . 30 , madre , affetto , ci rimpiango , vergognare problema sessualità , ...</td>\n",
       "      <td>figlio  , piccolo  insegnare  baciare  labbro  , fare  . 30  anno  , madre  fare  , affetto  , ci  essere  rimpiangere  , vergognare  problema  sessualità  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Per tutti i followers...... non sono il front man dei Dari, sono un informatico ed ho 42 anni, il Dario che cercate é un mio omonimo.</td>\n",
       "      <td>il followers ...... front man Dari , informatico 42 , Dario cercare é omonimo .</td>\n",
       "      <td>il  followers...... front  mano  Dari  , informatico  42  anno  , Dario  cercare  essere  omonimo  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!</td>\n",
       "      <td>51 capitare provare odio !</td>\n",
       "      <td>51  capitare  provare  odio  governo  !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@carmelitadurso ciao barbara  ho ho seguito l'intervento di lemme io sono grassa ho 53 anni sono pronta a mettermi in gioco se mi seguì tu</td>\n",
       "      <td>ciao Barbara   intervento lemme grasso 53 pronto mettere mi gioco seguire</td>\n",
       "      <td>ciao  barbara  il  intervento  lemme  grasso  53  pronto  mettere  mi  gioco  seguire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PietroF70 Diplomato alla scuola alberghiera,ho fatto aiuto cuoco, lavapiatti,portiere notturno e istruttore di scuola guida e perché ho 42 anni a casa</td>\n",
       "      <td>diplomato scuola alberghiero , aiuto cuoco , lavapiatto , portiere notturno istruttore scuola guidare 42</td>\n",
       "      <td>diplomare  scuola  alberghiero  , avere  aiuto  cuoco  , lavapiatto  , portiere  notturno  istruttore  scuola  guida  42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>Mi chiamo Marianna, ho 28 anni e  vorrei credere nel mio voto. Ma ahimè, tristemente andrò a votare quello che fa meno schifo, forse.</td>\n",
       "      <td>chare Marianna , 28   volere credere voto . , tristemente andrò votare schifo , .</td>\n",
       "      <td>chiamare  Marianna  , 28  volere  credere  voto  . ahimè  , tristemente  andare  votare  schifo  , forse  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>@civati Populismo di sinistra, populismo di destraAggrappati all’unica vera figura seria, l’attuale presidente del consiglio.Non credo di aver mai letto/sentito da quando sono nato (e ho 47 anni):”facciamo come l’Italia” detto dai tedeschi. Ci vuole preparazione/cultura per fare politica</td>\n",
       "      <td>populismo sinistra , populismo destraAggrappare a il unico vero figura serio , il attuale presidente . credere letto / sentire nato ( 47 anni):”faccare il Italia ” tedesco . volere preparazione / cultura politico</td>\n",
       "      <td>populismo  sinistra  , populismo  destro  aggrappare  a  il  unico  vero  figura  serio  , il  attuale  presidente  consiglio  . non  credere  letto/sentito  nascere  (  e  47  anno  )  :  ”facciare  il  Italia  ”  tedere  hi  . volere  preparazione  /cultura  politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>@rosita17rosita ho 49anni e vorrei diventare un politico senza compenso solo per risolvere i problemi dell'Italia</td>\n",
       "      <td>49anni volere politico compenso risolvere il problema Italia</td>\n",
       "      <td>49anni  volere  politico  compenso  risolvere  il  problema  di  il  Italia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                original  \\\n",
       "0     @StefanoGuerrera Sono una figlia, da piccola non mi hanno insegnato a baciare sulle labbra, mai fatto. Ora ho 30 anni, qualche volta con mia madre lo facevo, per affetto, ed ora che non c’è più lo rimpiango, mai vergognata e mai avuto problemi di sessualità, se ci fosse ancora lo farei ...   \n",
       "1                                                                                                                                                                  Per tutti i followers...... non sono il front man dei Dari, sono un informatico ed ho 42 anni, il Dario che cercate é un mio omonimo.   \n",
       "2                                                                                                                                                                                                           @ladyonorato Ho 51 anni e non mi è mai capitato di provare così tanto odio verso il governo!   \n",
       "3                                                                                                                                                             @carmelitadurso ciao barbara  ho ho seguito l'intervento di lemme io sono grassa ho 53 anni sono pronta a mettermi in gioco se mi seguì tu   \n",
       "4                                                                                                                                                @PietroF70 Diplomato alla scuola alberghiera,ho fatto aiuto cuoco, lavapiatti,portiere notturno e istruttore di scuola guida e perché ho 42 anni a casa   \n",
       "...                                                                                                                                                                                                                                                                                                  ...   \n",
       "1114                                                                                                                                                               Mi chiamo Marianna, ho 28 anni e  vorrei credere nel mio voto. Ma ahimè, tristemente andrò a votare quello che fa meno schifo, forse.   \n",
       "1115    @civati Populismo di sinistra, populismo di destraAggrappati all’unica vera figura seria, l’attuale presidente del consiglio.Non credo di aver mai letto/sentito da quando sono nato (e ho 47 anni):”facciamo come l’Italia” detto dai tedeschi. Ci vuole preparazione/cultura per fare politica   \n",
       "1116                                                                                                                                                                                                                                                                                                 nan   \n",
       "1117                                                                                                                                                                                   @rosita17rosita ho 49anni e vorrei diventare un politico senza compenso solo per risolvere i problemi dell'Italia   \n",
       "1118                                                                                                                                                                                                                                                                                                 nan   \n",
       "\n",
       "                                                                                                                                                                                                          lemmatized_spacy  \\\n",
       "0                                                                                                 figlio , piccolo insegnato baciare labbro , . 30 , madre , affetto , ci rimpiango , vergognare problema sessualità , ...   \n",
       "1                                                                                                                                          il followers ...... front man Dari , informatico 42 , Dario cercare é omonimo .   \n",
       "2                                                                                                                                                                                               51 capitare provare odio !   \n",
       "3                                                                                                                                                ciao Barbara   intervento lemme grasso 53 pronto mettere mi gioco seguire   \n",
       "4                                                                                                                 diplomato scuola alberghiero , aiuto cuoco , lavapiatto , portiere notturno istruttore scuola guidare 42   \n",
       "...                                                                                                                                                                                                                    ...   \n",
       "1114                                                                                                                                     chare Marianna , 28   volere credere voto . , tristemente andrò votare schifo , .   \n",
       "1115  populismo sinistra , populismo destraAggrappare a il unico vero figura serio , il attuale presidente . credere letto / sentire nato ( 47 anni):”faccare il Italia ” tedesco . volere preparazione / cultura politico   \n",
       "1116                                                                                                                                                                                                                   nan   \n",
       "1117                                                                                                                                                          49anni volere politico compenso risolvere il problema Italia   \n",
       "1118                                                                                                                                                                                                                   nan   \n",
       "\n",
       "                                                                                                                                                                                                                                                                   lemmatized_stanza  \n",
       "0                                                                                                                  figlio  , piccolo  insegnare  baciare  labbro  , fare  . 30  anno  , madre  fare  , affetto  , ci  essere  rimpiangere  , vergognare  problema  sessualità  , ...  \n",
       "1                                                                                                                                                                               il  followers...... front  mano  Dari  , informatico  42  anno  , Dario  cercare  essere  omonimo  .  \n",
       "2                                                                                                                                                                                                                                           51  capitare  provare  odio  governo  !   \n",
       "3                                                                                                                                                                                             ciao  barbara  il  intervento  lemme  grasso  53  pronto  mettere  mi  gioco  seguire   \n",
       "4                                                                                                                                                          diplomare  scuola  alberghiero  , avere  aiuto  cuoco  , lavapiatto  , portiere  notturno  istruttore  scuola  guida  42   \n",
       "...                                                                                                                                                                                                                                                                              ...  \n",
       "1114                                                                                                                                                                     chiamare  Marianna  , 28  volere  credere  voto  . ahimè  , tristemente  andare  votare  schifo  , forse  .  \n",
       "1115  populismo  sinistra  , populismo  destro  aggrappare  a  il  unico  vero  figura  serio  , il  attuale  presidente  consiglio  . non  credere  letto/sentito  nascere  (  e  47  anno  )  :  ”facciare  il  Italia  ”  tedere  hi  . volere  preparazione  /cultura  politico   \n",
       "1116                                                                                                                                                                                                                                                                            nan   \n",
       "1117                                                                                                                                                                                                    49anni  volere  politico  compenso  risolvere  il  problema  di  il  Italia   \n",
       "1118                                                                                                                                                                                                                                                                            nan   \n",
       "\n",
       "[1119 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc50456-0061-4d76-858b-9f713ee8c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to find retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b261fa49-28b8-4e5d-b799-c3fdc863b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('/g100_work/IscrC_mental/data/database/MENTALISM.db')\n",
    "your_query = \"SELECT * FROM tweets limit 100000\"\n",
    "df = pd.read_sql_query(your_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c5e6984-7e1e-4684-a4f2-825e125cd2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['text'] = df['text'].fillna('')\n",
    "retweet_df = df[df['text'].str.match(r'^RT \\w+:')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95073ca6-614d-4b3b-bd1c-1f762dd947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_text</th>\n",
       "      <th>language</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet_id, user_id, created_at, text, retweet_text, language, likes, retweets]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b533820-349e-4fbd-b9e3-df5d1d6015c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dff \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlemmas_spacy_stanza_copy.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/jupyter/interactive/rel-22.10/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "dff = pd.read_csv('lemmas_spacy_stanza_copy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

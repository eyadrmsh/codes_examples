{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0f2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ba1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = ''\n",
    "ats = ''\n",
    "bt = ''\n",
    "\n",
    "api_key = ''\n",
    "api_secret_key = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a12f3f",
   "metadata": {},
   "source": [
    "Creating time period for gathering tweets, after collecting counts for all tweets within segments of 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [str(i)[:-9] + 'T00:00:00Z' for i in pd.date_range(start='11/20/2022', periods=150)] # initialising 150 days period \n",
    "\n",
    "MY_BEARER_TOKEN = bt\n",
    "client = tweepy.Client(bearer_token=MY_BEARER_TOKEN)\n",
    "\n",
    "master = []\n",
    "\n",
    "for i in range(0, len(dates)//30 + 2):\n",
    "    data2 = client.get_all_tweets_count(query='balenciaga',\n",
    "                         start_time=dates[i * 30],\n",
    "                         end_time=dates[(i+1) * 30]\n",
    "                         )\n",
    "    \n",
    "    master.append(pd.DataFrame(data2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080c7b2",
   "metadata": {},
   "source": [
    "Here one should watch for the \"TooManyRequests: 429\" error which occurs when the client has exceeded the rate limit of the API service that we're using. API providers often impose rate limits to prevent abuse and ensure fair usage of their services. If you exceed the rate limit, the API will return a \"TooManyRequests\" error. To resolve this error, one should wait for some time (usually a few minutes) before retrying the request. The exact duration of the wait time depends on the specific API service and rate limit policy. The most common request limit interval is fifteen minutes. Our endpoint has a rate limit of 300 requests/15 minutes, then up to 300 requests over any 15-minute interval are allowed. Our code will handle rate limiting properly by retrying the request after waiting for 900 seconds (15 minutes) using time.sleep() when it reaches the maximum requests possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pairs_arr = pd.DataFrame({'start':dates[:-1], 'end':dates[1:]})\n",
    "\n",
    "\n",
    "MY_BEARER_TOKEN = bt\n",
    "client = tweepy.Client(bearer_token=MY_BEARER_TOKEN)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for search_query in tqdm(['balenciaga']):\n",
    "    for i in range(pairs_arr.shape[0]):\n",
    "\n",
    "        try:\n",
    "            '''\n",
    "            fetches tweet counts for the specified search query ('balenciaga')\n",
    "            within the specified time range (defined by start_time and end_time) with a granularity of 'minute'.\n",
    "            '''\n",
    "            tweets = client.get_all_tweets_count(query=search_query,\n",
    "                                                 start_time=pairs_arr['start'][i],\n",
    "                                                 end_time=pairs_arr['end'][i],\n",
    "                                                 granularity='minute'\n",
    "                                                 )\n",
    "            df = pd.DataFrame(tweets.data)\n",
    "            print(search_query, df['tweet_count'].sum())\n",
    "\n",
    "            df['search_query'] = search_query\n",
    "\n",
    "            counter += df['tweet_count'].sum()\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Waiting mode')\n",
    "            time.sleep(900)\n",
    "\n",
    "            '''\n",
    "            after sleeping we are trying to fetch tweets counts again\n",
    "            '''\n",
    "\n",
    "            tweets = client.get_all_tweets_count(query=search_query,\n",
    "                                                 start_time=pair[0],\n",
    "                                                 end_time=pair[1],\n",
    "                                                 granularity='minute'\n",
    "                                                 )\n",
    "            df = pd.DataFrame(tweets.data)\n",
    "            print(search_query, df['tweet_count'].sum())\n",
    "\n",
    "            df['search_query'] = search_query\n",
    "\n",
    "            counter += df['tweet_count'].sum()\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, axis=0)\n",
    "dfs.to_pickle('time_ranges1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc48926",
   "metadata": {},
   "source": [
    "The purpose of this loop is to assign each row to a specific time period based on the number of tweets associated with the tag within that time period. In general, it is grouping together contiguous time periods. There is a 500-requests limit, so first you need to collect the time ranges and then aggregate them to have the most optimal time intervals 500 tweets per request limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288df792",
   "metadata": {},
   "outputs": [],
   "source": [
    "### categorizes tweet counts into different time periods\n",
    "### counter2  is used to track the cumulative tweet count over multiple time periods\n",
    "### counter for  the tweet count within a single time period.\n",
    "\n",
    "tags = ['balenciaga']\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for tag in tags:\n",
    "    df_local = dfs.copy().reset_index(drop=True)\n",
    "\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "\n",
    "    for i in tqdm(range(1, df_local.shape[0])):\n",
    "        counter2 += df_local['tweet_count'][i]\n",
    "        \n",
    "        if df_local['tweet_count'][i] >= 450:\n",
    "            if df_local['tweet_count'][i-1] < 450:\n",
    "                df_local.at[i-1, 'timeperiod'] = 1\n",
    "            df_local.at[i, 'timeperiod'] = 1\n",
    "            counter = 0\n",
    "            counter2 = 0\n",
    "        elif counter2 >= 450 and counter < 450 and counter > 0:\n",
    "            df_local.at[i-1, 'timeperiod'] = 1\n",
    "            counter = df_local['tweet_count'][i]\n",
    "            counter2 = df_local['tweet_count'][i]\n",
    "        elif counter2 < 450 and counter < 450:\n",
    "            counter += df_local.tweet_count[i]\n",
    "        elif counter == 0 and counter2 > 450:\n",
    "            df_local.at[i, 'timeperiod'] = 2\n",
    "            counter = 0\n",
    "            counter2 = 0\n",
    "        \n",
    "    df_all.append(df_local)\n",
    "    \n",
    "df_all = pd.concat(df_all, axis = 0)\n",
    "df_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_all2 = []\n",
    "\n",
    "\n",
    "df_local = df_all_[df_all_['search_query'] == tag].reset_index(drop=True)\n",
    "ends = df_local['end'].values.tolist()[:-1]\n",
    "starts = [df_local['start'][0]] + ends\n",
    "df_local['start'] = starts\n",
    "df_all2.append(df_local)\n",
    "    \n",
    "df_all2 = pd.concat(df_all2, axis=0)\n",
    "df_all2.to_pickle('intervals.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8544e6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end</th>\n",
       "      <th>start</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>search_query</th>\n",
       "      <th>timeperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-20T00:01:00.000Z</td>\n",
       "      <td>2022-11-20T00:00:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-20T02:09:00.000Z</td>\n",
       "      <td>2022-11-20T00:01:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-20T03:37:00.000Z</td>\n",
       "      <td>2022-11-20T02:09:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-20T04:39:00.000Z</td>\n",
       "      <td>2022-11-20T03:37:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-20T06:28:00.000Z</td>\n",
       "      <td>2022-11-20T04:39:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6257</th>\n",
       "      <td>2023-04-11T13:40:00.000Z</td>\n",
       "      <td>2023-04-11T10:55:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6258</th>\n",
       "      <td>2023-04-11T16:00:00.000Z</td>\n",
       "      <td>2023-04-11T13:40:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6259</th>\n",
       "      <td>2023-04-11T18:51:00.000Z</td>\n",
       "      <td>2023-04-11T16:00:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6260</th>\n",
       "      <td>2023-04-11T22:11:00.000Z</td>\n",
       "      <td>2023-04-11T18:51:00.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6261</th>\n",
       "      <td>2023-04-12T00:00:00.000Z</td>\n",
       "      <td>2023-04-11T22:11:00.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6262 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           end                     start  tweet_count  \\\n",
       "0     2022-11-20T00:01:00.000Z  2022-11-20T00:00:00.000Z            4   \n",
       "1     2022-11-20T02:09:00.000Z  2022-11-20T00:01:00.000Z            5   \n",
       "2     2022-11-20T03:37:00.000Z  2022-11-20T02:09:00.000Z            5   \n",
       "3     2022-11-20T04:39:00.000Z  2022-11-20T03:37:00.000Z            4   \n",
       "4     2022-11-20T06:28:00.000Z  2022-11-20T04:39:00.000Z            5   \n",
       "...                        ...                       ...          ...   \n",
       "6257  2023-04-11T13:40:00.000Z  2023-04-11T10:55:00.000Z            4   \n",
       "6258  2023-04-11T16:00:00.000Z  2023-04-11T13:40:00.000Z            4   \n",
       "6259  2023-04-11T18:51:00.000Z  2023-04-11T16:00:00.000Z            5   \n",
       "6260  2023-04-11T22:11:00.000Z  2023-04-11T18:51:00.000Z            2   \n",
       "6261  2023-04-12T00:00:00.000Z  2023-04-11T22:11:00.000Z            1   \n",
       "\n",
       "     search_query  timeperiod  \n",
       "0      balenciaga         1.0  \n",
       "1      balenciaga         1.0  \n",
       "2      balenciaga         1.0  \n",
       "3      balenciaga         1.0  \n",
       "4      balenciaga         1.0  \n",
       "...           ...         ...  \n",
       "6257   balenciaga         1.0  \n",
       "6258   balenciaga         1.0  \n",
       "6259   balenciaga         1.0  \n",
       "6260   balenciaga         1.0  \n",
       "6261   balenciaga         1.0  \n",
       "\n",
       "[6262 rows x 5 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "583735c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end</th>\n",
       "      <th>start</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>search_query</th>\n",
       "      <th>timeperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-20T00:01:00.000Z</td>\n",
       "      <td>2022-11-20T00:00:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-20T02:09:00.000Z</td>\n",
       "      <td>2022-11-20T00:01:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-20T03:37:00.000Z</td>\n",
       "      <td>2022-11-20T02:09:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-20T04:39:00.000Z</td>\n",
       "      <td>2022-11-20T03:37:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-20T06:28:00.000Z</td>\n",
       "      <td>2022-11-20T04:39:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6257</th>\n",
       "      <td>2023-04-11T13:40:00.000Z</td>\n",
       "      <td>2023-04-11T10:55:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6258</th>\n",
       "      <td>2023-04-11T16:00:00.000Z</td>\n",
       "      <td>2023-04-11T13:40:00.000Z</td>\n",
       "      <td>4</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6259</th>\n",
       "      <td>2023-04-11T18:51:00.000Z</td>\n",
       "      <td>2023-04-11T16:00:00.000Z</td>\n",
       "      <td>5</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6260</th>\n",
       "      <td>2023-04-11T22:11:00.000Z</td>\n",
       "      <td>2023-04-11T18:51:00.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6261</th>\n",
       "      <td>2023-04-12T00:00:00.000Z</td>\n",
       "      <td>2023-04-11T22:11:00.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>balenciaga</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6262 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           end                     start  tweet_count  \\\n",
       "0     2022-11-20T00:01:00.000Z  2022-11-20T00:00:00.000Z            4   \n",
       "1     2022-11-20T02:09:00.000Z  2022-11-20T00:01:00.000Z            5   \n",
       "2     2022-11-20T03:37:00.000Z  2022-11-20T02:09:00.000Z            5   \n",
       "3     2022-11-20T04:39:00.000Z  2022-11-20T03:37:00.000Z            4   \n",
       "4     2022-11-20T06:28:00.000Z  2022-11-20T04:39:00.000Z            5   \n",
       "...                        ...                       ...          ...   \n",
       "6257  2023-04-11T13:40:00.000Z  2023-04-11T10:55:00.000Z            4   \n",
       "6258  2023-04-11T16:00:00.000Z  2023-04-11T13:40:00.000Z            4   \n",
       "6259  2023-04-11T18:51:00.000Z  2023-04-11T16:00:00.000Z            5   \n",
       "6260  2023-04-11T22:11:00.000Z  2023-04-11T18:51:00.000Z            2   \n",
       "6261  2023-04-12T00:00:00.000Z  2023-04-11T22:11:00.000Z            1   \n",
       "\n",
       "     search_query  timeperiod  \n",
       "0      balenciaga         1.0  \n",
       "1      balenciaga         1.0  \n",
       "2      balenciaga         1.0  \n",
       "3      balenciaga         1.0  \n",
       "4      balenciaga         1.0  \n",
       "...           ...         ...  \n",
       "6257   balenciaga         1.0  \n",
       "6258   balenciaga         1.0  \n",
       "6259   balenciaga         1.0  \n",
       "6260   balenciaga         1.0  \n",
       "6261   balenciaga         1.0  \n",
       "\n",
       "[6262 rows x 5 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = pd.read_pickle('intervals.pkl')\n",
    "ch.reset_index(inplace=True, drop=True)\n",
    "ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46c780",
   "metadata": {},
   "source": [
    "In the next approach, the code iterates over each row to find tweets’ authors, their creation time, their text, the language of texts, their retweet counts, their reply counts, and their locations.\n",
    "Endpoint has a rate limit of 300 requests/15 minutes, then up to 300 requests over any 15-minute interval are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1309f822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575be7e191c42a8b40d8d8e79fd4754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-26T11:58:00.000Z 2023-03-26T15:19:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-03-26T15:19:00.000Z 2023-03-26T17:59:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-26T17:59:00.000Z 2023-03-26T20:05:00.000Z\n",
      "Было найдено еще 427 твитов\n",
      "2023-03-26T20:05:00.000Z 2023-03-26T22:10:00.000Z\n",
      "Было найдено еще 413 твитов\n",
      "2023-03-26T22:10:00.000Z 2023-03-27T00:02:00.000Z\n",
      "Было найдено еще 418 твитов\n",
      "2023-03-27T00:02:00.000Z 2023-03-27T01:49:00.000Z\n",
      "Было найдено еще 428 твитов\n",
      "2023-03-27T01:49:00.000Z 2023-03-27T03:34:00.000Z\n",
      "Было найдено еще 430 твитов\n",
      "2023-03-27T03:34:00.000Z 2023-03-27T05:35:00.000Z\n",
      "Было найдено еще 429 твитов\n",
      "2023-03-27T05:35:00.000Z 2023-03-27T07:53:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-03-27T07:53:00.000Z 2023-03-27T09:49:00.000Z\n",
      "Было найдено еще 436 твитов\n",
      "2023-03-27T09:49:00.000Z 2023-03-27T11:29:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-03-27T11:29:00.000Z 2023-03-27T13:11:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-27T13:11:00.000Z 2023-03-27T15:03:00.000Z\n",
      "Было найдено еще 446 твитов\n",
      "2023-03-27T15:03:00.000Z 2023-03-27T16:58:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-03-27T16:58:00.000Z 2023-03-27T18:46:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-03-27T18:46:00.000Z 2023-03-27T20:06:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-03-27T20:06:00.000Z 2023-03-27T22:07:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-27T22:07:00.000Z 2023-03-28T00:41:00.000Z\n",
      "Было найдено еще 413 твитов\n",
      "2023-03-28T00:41:00.000Z 2023-03-28T02:59:00.000Z\n",
      "Было найдено еще 401 твитов\n",
      "2023-03-28T02:59:00.000Z 2023-03-28T05:49:00.000Z\n",
      "Было найдено еще 404 твитов\n",
      "2023-03-28T05:49:00.000Z 2023-03-28T09:26:00.000Z\n",
      "Было найдено еще 391 твитов\n",
      "2023-03-28T09:26:00.000Z 2023-03-28T12:37:00.000Z\n",
      "Было найдено еще 414 твитов\n",
      "2023-03-28T12:37:00.000Z 2023-03-28T14:47:00.000Z\n",
      "Было найдено еще 423 твитов\n",
      "2023-03-28T14:47:00.000Z 2023-03-28T16:37:00.000Z\n",
      "Было найдено еще 429 твитов\n",
      "2023-03-28T16:37:00.000Z 2023-03-28T18:36:00.000Z\n",
      "Было найдено еще 436 твитов\n",
      "2023-03-28T18:36:00.000Z 2023-03-28T20:31:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-03-28T20:31:00.000Z 2023-03-28T23:16:00.000Z\n",
      "Было найдено еще 425 твитов\n",
      "2023-03-28T23:16:00.000Z 2023-03-29T02:43:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-29T02:43:00.000Z 2023-03-29T06:08:00.000Z\n",
      "Было найдено еще 427 твитов\n",
      "2023-03-29T06:08:00.000Z 2023-03-29T10:17:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-29T10:17:00.000Z 2023-03-29T14:11:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-03-29T14:11:00.000Z 2023-03-29T16:43:00.000Z\n",
      "Было найдено еще 424 твитов\n",
      "2023-03-29T16:43:00.000Z 2023-03-29T20:52:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-03-29T20:52:00.000Z 2023-03-30T00:46:00.000Z\n",
      "Было найдено еще 362 твитов\n",
      "2023-03-30T00:46:00.000Z 2023-03-30T05:33:00.000Z\n",
      "Было найдено еще 394 твитов\n",
      "2023-03-30T05:33:00.000Z 2023-03-30T08:44:00.000Z\n",
      "Было найдено еще 428 твитов\n",
      "2023-03-30T08:44:00.000Z 2023-03-30T10:37:00.000Z\n",
      "Было найдено еще 428 твитов\n",
      "2023-03-30T10:37:00.000Z 2023-03-30T13:28:00.000Z\n",
      "Было найдено еще 420 твитов\n",
      "2023-03-30T13:28:00.000Z 2023-03-30T16:37:00.000Z\n",
      "Было найдено еще 416 твитов\n",
      "2023-03-30T16:37:00.000Z 2023-03-30T20:31:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-03-30T20:31:00.000Z 2023-03-30T23:51:00.000Z\n",
      "Было найдено еще 428 твитов\n",
      "2023-03-30T23:51:00.000Z 2023-03-31T03:14:00.000Z\n",
      "Было найдено еще 430 твитов\n",
      "2023-03-31T03:14:00.000Z 2023-03-31T07:51:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-03-31T07:51:00.000Z 2023-03-31T09:05:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-03-31T09:05:00.000Z 2023-03-31T10:35:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-03-31T10:35:00.000Z 2023-03-31T12:33:00.000Z\n",
      "Было найдено еще 447 твитов\n",
      "2023-03-31T12:33:00.000Z 2023-03-31T14:26:00.000Z\n",
      "Было найдено еще 446 твитов\n",
      "2023-03-31T14:26:00.000Z 2023-03-31T16:56:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-03-31T16:56:00.000Z 2023-03-31T19:22:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-03-31T19:22:00.000Z 2023-03-31T23:05:00.000Z\n",
      "Было найдено еще 436 твитов\n",
      "2023-03-31T23:05:00.000Z 2023-04-01T03:56:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-01T03:56:00.000Z 2023-04-01T09:35:00.000Z\n",
      "Было найдено еще 436 твитов\n",
      "2023-04-01T09:35:00.000Z 2023-04-01T13:50:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-04-01T13:50:00.000Z 2023-04-01T17:11:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-01T17:11:00.000Z 2023-04-01T20:17:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-01T20:17:00.000Z 2023-04-01T23:30:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-01T23:30:00.000Z 2023-04-02T02:26:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-02T02:26:00.000Z 2023-04-02T05:46:00.000Z\n",
      "Было найдено еще 445 твитов\n",
      "2023-04-02T05:46:00.000Z 2023-04-02T08:13:00.000Z\n",
      "Было найдено еще 447 твитов\n",
      "2023-04-02T08:13:00.000Z 2023-04-02T11:57:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-02T11:57:00.000Z 2023-04-02T14:36:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-02T14:36:00.000Z 2023-04-02T17:24:00.000Z\n",
      "Было найдено еще 442 твитов\n",
      "2023-04-02T17:24:00.000Z 2023-04-02T20:31:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-02T20:31:00.000Z 2023-04-02T23:36:00.000Z\n",
      "Было найдено еще 434 твитов\n",
      "2023-04-02T23:36:00.000Z 2023-04-03T03:37:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-03T03:37:00.000Z 2023-04-03T08:54:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-03T08:54:00.000Z 2023-04-03T12:16:00.000Z\n",
      "Было найдено еще 442 твитов\n",
      "2023-04-03T12:16:00.000Z 2023-04-03T15:31:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-03T15:31:00.000Z 2023-04-03T18:13:00.000Z\n",
      "Было найдено еще 442 твитов\n",
      "2023-04-03T18:13:00.000Z 2023-04-03T22:10:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-03T22:10:00.000Z 2023-04-04T01:11:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-04T01:11:00.000Z 2023-04-04T04:36:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-04T04:36:00.000Z 2023-04-04T07:54:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-04T07:54:00.000Z 2023-04-04T10:59:00.000Z\n",
      "Было найдено еще 424 твитов\n",
      "2023-04-04T10:59:00.000Z 2023-04-04T14:10:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-04T14:10:00.000Z 2023-04-04T16:29:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-04T16:29:00.000Z 2023-04-04T19:32:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-04T19:32:00.000Z 2023-04-04T21:53:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-04T21:53:00.000Z 2023-04-04T23:16:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-04T23:16:00.000Z 2023-04-05T00:56:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-05T00:56:00.000Z 2023-04-05T02:44:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-04-05T02:44:00.000Z 2023-04-05T04:26:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-05T04:26:00.000Z 2023-04-05T06:38:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-05T06:38:00.000Z 2023-04-05T09:10:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-05T09:10:00.000Z 2023-04-05T11:22:00.000Z\n",
      "Было найдено еще 445 твитов\n",
      "2023-04-05T11:22:00.000Z 2023-04-05T13:03:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-05T13:03:00.000Z 2023-04-05T14:32:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-05T14:32:00.000Z 2023-04-05T15:39:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-05T15:39:00.000Z 2023-04-05T17:04:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-05T17:04:00.000Z 2023-04-05T18:37:00.000Z\n",
      "Было найдено еще 430 твитов\n",
      "2023-04-05T18:37:00.000Z 2023-04-05T20:15:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-05T20:15:00.000Z 2023-04-05T22:26:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-05T22:26:00.000Z 2023-04-06T01:27:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-06T01:27:00.000Z 2023-04-06T04:51:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-04-06T04:51:00.000Z 2023-04-06T08:26:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-06T08:26:00.000Z 2023-04-06T12:11:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-06T12:11:00.000Z 2023-04-06T14:34:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-06T14:34:00.000Z 2023-04-06T17:08:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-06T17:08:00.000Z 2023-04-06T19:30:00.000Z\n",
      "Было найдено еще 441 твитов\n",
      "2023-04-06T19:30:00.000Z 2023-04-06T22:55:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-06T22:55:00.000Z 2023-04-07T02:35:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-07T02:35:00.000Z 2023-04-07T05:46:00.000Z\n",
      "Было найдено еще 426 твитов\n",
      "2023-04-07T05:46:00.000Z 2023-04-07T08:30:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-07T08:30:00.000Z 2023-04-07T11:07:00.000Z\n",
      "Было найдено еще 442 твитов\n",
      "2023-04-07T11:07:00.000Z 2023-04-07T13:45:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-07T13:45:00.000Z 2023-04-07T15:52:00.000Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Было найдено еще 437 твитов\n",
      "2023-04-07T15:52:00.000Z 2023-04-07T17:47:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-04-07T17:47:00.000Z 2023-04-07T19:45:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-07T19:45:00.000Z 2023-04-07T21:30:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-04-07T21:30:00.000Z 2023-04-07T23:14:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-07T23:14:00.000Z 2023-04-08T01:40:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-08T01:40:00.000Z 2023-04-08T05:08:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-08T05:08:00.000Z 2023-04-08T07:56:00.000Z\n",
      "Было найдено еще 442 твитов\n",
      "2023-04-08T07:56:00.000Z 2023-04-08T11:06:00.000Z\n",
      "Было найдено еще 436 твитов\n",
      "2023-04-08T11:06:00.000Z 2023-04-08T14:24:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-04-08T14:24:00.000Z 2023-04-08T17:23:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-08T17:23:00.000Z 2023-04-08T20:19:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-08T20:19:00.000Z 2023-04-08T22:56:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-04-08T22:56:00.000Z 2023-04-09T02:24:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-09T02:24:00.000Z 2023-04-09T05:48:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-09T05:48:00.000Z 2023-04-09T10:44:00.000Z\n",
      "Было найдено еще 447 твитов\n",
      "2023-04-09T10:44:00.000Z 2023-04-09T14:35:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-09T14:35:00.000Z 2023-04-09T16:24:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-09T16:24:00.000Z 2023-04-09T18:22:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-09T18:22:00.000Z 2023-04-09T21:06:00.000Z\n",
      "Было найдено еще 443 твитов\n",
      "2023-04-09T21:06:00.000Z 2023-04-10T00:42:00.000Z\n",
      "Было найдено еще 438 твитов\n",
      "2023-04-10T00:42:00.000Z 2023-04-10T05:06:00.000Z\n",
      "Было найдено еще 439 твитов\n",
      "2023-04-10T05:06:00.000Z 2023-04-10T09:07:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-04-10T09:07:00.000Z 2023-04-10T13:08:00.000Z\n",
      "Было найдено еще 428 твитов\n",
      "2023-04-10T13:08:00.000Z 2023-04-10T14:22:00.000Z\n",
      "Было найдено еще 444 твитов\n",
      "2023-04-10T14:22:00.000Z 2023-04-10T17:44:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-10T17:44:00.000Z 2023-04-10T20:52:00.000Z\n",
      "Было найдено еще 433 твитов\n",
      "2023-04-10T20:52:00.000Z 2023-04-11T00:42:00.000Z\n",
      "Было найдено еще 445 твитов\n",
      "2023-04-11T00:42:00.000Z 2023-04-11T06:32:00.000Z\n",
      "Было найдено еще 440 твитов\n",
      "2023-04-11T06:32:00.000Z 2023-04-11T10:55:00.000Z\n",
      "Было найдено еще 437 твитов\n",
      "2023-04-11T10:55:00.000Z 2023-04-11T13:40:00.000Z\n",
      "Было найдено еще 435 твитов\n",
      "2023-04-11T13:40:00.000Z 2023-04-11T16:00:00.000Z\n",
      "Было найдено еще 425 твитов\n",
      "2023-04-11T16:00:00.000Z 2023-04-11T18:51:00.000Z\n",
      "Было найдено еще 382 твитов\n",
      "2023-04-11T18:51:00.000Z 2023-04-11T22:11:00.000Z\n",
      "Было найдено еще 409 твитов\n",
      "2023-04-11T22:11:00.000Z 2023-04-12T00:00:00.000Z\n",
      "Было найдено еще 187 твитов\n"
     ]
    }
   ],
   "source": [
    "MY_BEARER_TOKEN = bt\n",
    "import time\n",
    "\n",
    "def extract_geo(s):\n",
    "    try:\n",
    "        a = s['place_id']\n",
    "        return a\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# df_arr_all = []\n",
    "counter = 0\n",
    "\n",
    "for i in tqdm(range(0, ch.shape[0])):\n",
    "    \n",
    "    print(ch['start'][i], ch['end'][i])\n",
    "    \n",
    "    try:\n",
    "        client = tweepy.Client(bearer_token=MY_BEARER_TOKEN)\n",
    "\n",
    "        tweets = client.search_all_tweets(query=ch['search_query'][i],\n",
    "                                         start_time=ch['start'][i],\n",
    "                                         end_time=ch['end'][i],\n",
    "                                         tweet_fields = [\"created_at\", \"text\", \"source\", 'author_id', 'public_metrics',\n",
    "                                                        'geo', 'entities', 'lang'],\n",
    "                                         user_fields = [\"name\", \"username\", \"location\", \"verified\", \"description\"],\n",
    "                                         place_fields = ['full_name', 'id', 'country', 'country_code', 'geo', 'name',\n",
    "                                                        'place_type'],\n",
    "                                         max_results = 500,\n",
    "                                         expansions=['author_id', 'geo.place_id']\n",
    "                                         )\n",
    "    except:\n",
    "        client = tweepy.Client(bearer_token=MY_BEARER_TOKEN)\n",
    "\n",
    "        tweets = client.search_all_tweets(query=ch['search_query'][i],\n",
    "                                         start_time=ch['start'][i],\n",
    "                                         end_time=ch['end'][i],\n",
    "                                         tweet_fields = [\"created_at\", \"text\", \"source\", 'author_id', 'public_metrics',\n",
    "                                                        'geo', 'entities', 'lang'],\n",
    "                                         user_fields = [\"name\", \"username\", \"location\", \"verified\", \"description\"],\n",
    "                                         place_fields = ['full_name', 'id', 'country', 'country_code', 'geo', 'name',\n",
    "                                                        'place_type'],\n",
    "                                         max_results = 500,\n",
    "                                         expansions=['author_id', 'geo.place_id']\n",
    "                                         )\n",
    "    counter += 1\n",
    "    time.sleep(1)\n",
    "    df_arr = []\n",
    "    if counter == 290:\n",
    "        time.sleep(900)\n",
    "        counter = 0\n",
    "\n",
    "    if tweets.meta['result_count'] > 0:\n",
    "\n",
    "        print(f\"Was found {tweets.meta['result_count']} tweets\")\n",
    "\n",
    "        for el in tweets.data:\n",
    "            df_arr.append([el.id, el.text, el.author_id, extract_geo(el.geo), el.created_at, \n",
    "                           el.lang, el.public_metrics.get('retweet_count', 0), el.public_metrics.get('reply_count', 0),\n",
    "                           el.public_metrics.get('like_count', 0), el.public_metrics.get('quote_count', 0)])\n",
    "\n",
    "        df = pd.DataFrame(df_arr)\n",
    "\n",
    "        df.columns = ['tweet_id', 'text', 'author_id', 'geo', 'created_at', 'lang', 'retweet_count', 'reply_count',\n",
    "                     'like_count', 'quote_count']\n",
    "\n",
    "        df['author_location'] = '-1'\n",
    "\n",
    "        df['country'] = 'No Country'\n",
    "\n",
    "        df['query'] = ch['search_query'][i]\n",
    "\n",
    "        df_arr_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c038f53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2284038, 13)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### saving tweets \n",
    "\n",
    "pd.concat(df_arr_all, axis=0).to_pickle('balenciaga2.pkl')\n",
    "dff = pd.concat(df_arr_all, axis=0)\n",
    "dff.shape\n",
    "\n",
    "### all in all 2284038 tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0909a1",
   "metadata": {},
   "source": [
    "Getting user data from batches for 100 users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "30865d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dff['author_id'].unique()\n",
    "df = pd.DataFrame({'author_id':users})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f\"Bearer {bt}\",\n",
    "}\n",
    "\n",
    "# profile_arr = []\n",
    "counter = 295\n",
    "\n",
    "for i in tqdm(range(0, df.shape[0] // 100 + 1)):\n",
    "    if counter == 295:\n",
    "        time.sleep(900)\n",
    "        counter = 0\n",
    "        \n",
    "    author_ids = ','.join([str(i) for i in df.iloc[i * 100:(i+1)*100, :]['author_id'].tolist()])\n",
    "    s = f\"https://api.twitter.com/2/users?ids={author_ids}&user.fields=location,name\"\n",
    "    response = requests.get(s, headers=headers)\n",
    "    r = json.loads(response.text)['data']\n",
    "    profile_arr.append(pd.DataFrame(r))\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4e4ba7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>withheld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nickithetaurus</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>1100213555767447556</td>\n",
       "      <td>Thow’d Off</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WindYCitYFreScO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186351277</td>\n",
       "      <td>Sir E G</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JoergBredemeier</td>\n",
       "      <td>Henstedt-Ulzburg, Deutschland</td>\n",
       "      <td>1522475722987786241</td>\n",
       "      <td>Jörg-H. Bredemeier</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemondreamsicle</td>\n",
       "      <td>NYC, Toronto or Lagos</td>\n",
       "      <td>4863079029</td>\n",
       "      <td>Creamsicle</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaybeBabii</td>\n",
       "      <td>pandora</td>\n",
       "      <td>3284443339</td>\n",
       "      <td>🏹Cooking 𝕄𝔹🏹</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                       location                   id  \\\n",
       "0   nickithetaurus                  Las Vegas, NV  1100213555767447556   \n",
       "1  WindYCitYFreScO                            NaN            186351277   \n",
       "2  JoergBredemeier  Henstedt-Ulzburg, Deutschland  1522475722987786241   \n",
       "3  lemondreamsicle          NYC, Toronto or Lagos           4863079029   \n",
       "4       MaybeBabii                        pandora           3284443339   \n",
       "\n",
       "                 name withheld  \n",
       "0          Thow’d Off      NaN  \n",
       "1             Sir E G      NaN  \n",
       "2  Jörg-H. Bredemeier      NaN  \n",
       "3          Creamsicle      NaN  \n",
       "4        🏹Cooking 𝕄𝔹🏹      NaN  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(profile_arr, axis = 0).reset_index(drop=True)\n",
    "df.to_pickle('users_balenciaga.pkl')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
